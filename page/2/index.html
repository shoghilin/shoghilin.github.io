<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Shoghi的隨手筆記">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Shoghi的隨手筆記">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Shoghi">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-TW'
  };
</script>

  <title>Shoghi的隨手筆記</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Shoghi的隨手筆記</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首頁</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>歸檔</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/29/Gradient-Descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/29/Gradient-Descent/" class="post-title-link" itemprop="url">Gradient-Descent</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-29 12:23:42 / 修改時間：16:43:24" itemprop="dateCreated datePublished" datetime="2020-07-29T12:23:42+08:00">2020-07-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient-Descent"></a>梯度下降 Gradient-Descent</h1><p>機器學習算法當中，優化算法的功能，是通過改善訓練方式，來最小化(或最大化)損失函數</p>
<h2 id="最常用的優化算法是Gradient-Descent"><a href="#最常用的優化算法是Gradient-Descent" class="headerlink" title="最常用的優化算法是Gradient Descent"></a>最常用的優化算法是Gradient Descent</h2><p>Gradient descent 是一個一階最佳化算法，通常也稱為最速下降法<br>目的：沿著目標函數梯度下降的方向搜索極小值（也可以沿著梯度上升的方向搜索極大值）<br>若要使用梯度下降法找到一個函數的局部極小值，必須向函數上當前點對應梯度（或者是近似梯度）的反方向的規定步長距離點進行疊代搜索。</p>
<h2 id="梯度下降法的缺點包括："><a href="#梯度下降法的缺點包括：" class="headerlink" title="梯度下降法的缺點包括："></a>梯度下降法的缺點包括：</h2><ul>
<li>靠近極小值時速度減慢</li>
<li>直線搜索可能會產生一些問題</li>
<li>可能會「之字型」地下降</li>
</ul>
<h2 id="學習率對梯度下降的影響"><a href="#學習率對梯度下降的影響" class="headerlink" title="學習率對梯度下降的影響"></a>學習率對梯度下降的影響</h2><p>學習率定義了每次疊代中應該更改的參數量。換句話說，它控制我們應該收斂到minimum的速度或速度<br>小學習率可以使迭代收斂，過大的學習率可能超過最小值<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294279/1592900888254" width = "400" height = "300" alt="learning rate" align=center /></p>
<h2 id="梯度下降法的過程"><a href="#梯度下降法的過程" class="headerlink" title="梯度下降法的過程"></a>梯度下降法的過程</h2><ol>
<li>首先需要設定一個初始參數值，通常情況下將初值設為零(w&#x3D;0)，</li>
<li>接下來需要計算成本函數 cost</li>
<li>然後計算函數的導數(某個點處的斜率值)，並設定學習效率參數(lr)的值</li>
<li>重複執行上述過程，直到參數值收斂，這樣我們就能獲得函數的最優解<img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294279/1592900924087" width = "400" height = "300" alt="gd-1" align=center /></li>
</ol>
<h2 id="要計算-Gradient-Descent，考慮"><a href="#要計算-Gradient-Descent，考慮" class="headerlink" title="要計算 Gradient Descent，考慮"></a>要計算 Gradient Descent，考慮</h2><ul>
<li>Loss &#x3D; 實際 ydata – 預測 ydata<br>Loss &#x3D;  w* 實際 xdata – w*預測 xdata (bias 為 init value，被消除)</li>
<li>Gradient &#x3D; ▽f (θ) (Gradient &#x3D; ∂L&#x2F;∂w)</li>
<li>調整後的權重 &#x3D; 原權重 – η(Learning rate) * Gradient</li>
</ul>
<h2 id="怎麼確定到極值點了呢？"><a href="#怎麼確定到極值點了呢？" class="headerlink" title="怎麼確定到極值點了呢？"></a>怎麼確定到極值點了呢？</h2><p>η又稱學習率，是一個挪動步長的基數，$\partial f(x)&#x2F;\partial x$是導函數，<br>當離極值很遠的時候導數大，移動的就快，當接近極值時，導數非常小，移動的就非常小</p>
<p><strong>然而，gradient descent並不保證可以達到global minimum，不同的起始點可以到達的minimum是不同的</strong></p>
<h2 id="梯度下降的算法調優"><a href="#梯度下降的算法調優" class="headerlink" title="梯度下降的算法調優"></a>梯度下降的算法調優</h2><p>Learning rate 選擇，實際上取值取決於數據樣本，如果損失函數在變小，說明取值有效，否則要增大 Learning rate</p>
<h2 id="自動更新-Learning-rate-衰減因子-decay"><a href="#自動更新-Learning-rate-衰減因子-decay" class="headerlink" title="自動更新 Learning rate  - 衰減因子 decay"></a>自動更新 Learning rate  - 衰減因子 decay</h2><p>算法參數的初始值選擇。初始值不同，獲得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；當然如果損失函數是凸函數則一定是最優解</p>
<h3 id="學習率衰減公式"><a href="#學習率衰減公式" class="headerlink" title="學習率衰減公式"></a>學習率衰減公式</h3><p>$$ lr_i &#x3D; lr_{start} * 1.0 &#x2F; (1.0 + decay * i)$$<br>其中 $lr_i$ 為第一迭代 i 時的學習率，$lr_start$ 為初始值，decay 為一個介於[0.0, 1.0]的小數。從公式上可看出：<br>decay 越小，學習率衰減地越慢，當 decay &#x3D; 0時，學習率保持不變<br>decay 越大，學習率衰減地越快，當 decay &#x3D; 1時，學習率衰減最快</p>
<h3 id="使用-momentum-是梯度下降法中一種常用的加速技術"><a href="#使用-momentum-是梯度下降法中一種常用的加速技術" class="headerlink" title="使用 momentum 是梯度下降法中一種常用的加速技術"></a>使用 momentum 是梯度下降法中一種常用的加速技術</h3><p>$x ← x − \alpha ∗ \partial x $(x沿負梯度方向下降)<br>$v &#x3D;  \beta ∗ v − a ∗ \partial x$<br>$x ← x + v$</p>
<p>其中 $\beta$ 即 momentum 係數，通俗的理解上面式子就是，如果上一次的momentum（即$\beta$ ）與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，所以這樣做能夠達到加速收斂的過程 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/27/activation-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/27/activation-function/" class="post-title-link" itemprop="url">activation function</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-27 17:23:09" itemprop="dateCreated datePublished" datetime="2020-07-27T17:23:09+08:00">2020-07-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-29 12:24:21" itemprop="dateModified" datetime="2020-07-29T12:24:21+08:00">2020-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="啟動函數-activation-function"><a href="#啟動函數-activation-function" class="headerlink" title="啟動函數 activation function"></a>啟動函數 activation function</h1><p>啟動函數定義了<br>每個節點（神經元）的輸出和輸入關係的函數為神經元提供規模化非線性化能力，讓神經網路具備強大的擬合能力</p>
<h2 id="啟動函數的作用"><a href="#啟動函數的作用" class="headerlink" title="啟動函數的作用"></a>啟動函數的作用</h2><ul>
<li><p>深度學習的基本原理是基於人工神經網路，信號從一個神經元進入，經過非線性的 activation function<br>如此循環往復，直到輸出層。正是由於這些非線性函數的反覆疊加，才使得神經網路有足夠的 capacity 來抓取複雜的 pattern</p>
</li>
<li><p>啟動函數的最大作用就是非線性化<br>如果不用啟動函數的話，無論神經網路有多少層，輸出都是輸入的線性組合</p>
</li>
<li><p>啟動函數的另一個重要特徵<br>它應該是可以區分forward-propagation與back-propagation的網路參數更新，然後相應地使用梯度下降或任何其他優化技術優化權重以減少誤差</p>
</li>
</ul>
<h2 id="常見啟動函數介紹"><a href="#常見啟動函數介紹" class="headerlink" title="常見啟動函數介紹"></a>常見啟動函數介紹</h2><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>特點是會把輸出限定在 0~1 之間，在 x&lt;0 ，輸出就是 0，在 x&gt;0，輸出就是 1，這樣使得數據在傳遞過程中不容易發散</p>
<h4 id="主要缺點"><a href="#主要缺點" class="headerlink" title="主要缺點"></a>主要缺點</h4><ul>
<li>Sigmoid 容易過飽和，丟失梯度。這樣在反向傳播時，很容易出現梯度消失的情況，導致訓練無法完整</li>
<li>Sigmoid 的輸出均值不是 0</li>
</ul>
<p>函式：<br>$$\sigma(x) &#x3D; \frac{1}{1+e^{-x}}$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899630177" width = "400" height = "300" alt="sigmoid" align=center /></p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>Softmax 把一個 k 維的 real value 向量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）其中 bi 是一個 0~1 的常數，輸出神經元之和為 1.0，所以也可以拿來做多分類的機率預測</p>
<h4 id="為什麼要取指數？"><a href="#為什麼要取指數？" class="headerlink" title="為什麼要取指數？"></a>為什麼要取指數？</h4><ul>
<li>要模擬 max 的行為，所以要讓大的更大</li>
<li>需要一個可導的函數</li>
</ul>
<p>函式：<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899676578" width = "300" height = "200" alt="softmax fomula" align=center /><br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899693493" width = "400" height = "300" alt="softmax" align=center /></p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>tanh 讀作 Hyperbolic Tangent<br>tanh 也稱為雙切正切函數，取值範圍為 [-1,1]</p>
<p>tanh 在特徵相差明顯時的效果會很好，在循環過程中會不斷擴大特徵效果</p>
<p>函式：<br>$$tanh(x)&#x3D;2\sigma(2x) - 1$$<br>$$tanh(x) &#x3D; \frac{e^x - e^{-x}}{e^x+e^{-x}}$$</p>
<img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899747636" width = "400" height = "300" alt="tanh" align=center />


<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>修正線性單元（Rectified linear unit，ReLU）</p>
<ul>
<li>在 x &gt; 0 時導數恆為 1</li>
<li>對於 x &lt; 0，其梯度恆為 0，這時候它也會出現飽和的現象，甚至使神經元直接無效，從而其權重無法得到更新（在這種情況下通常稱為 dying ReLU）</li>
<li>Leaky ReLU 和 PReLU 的提出正是為了解決這一問題</li>
</ul>
<p>函式：<br>$$f(x)&#x3D;max(0, x)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899800436" width = "400" height = "300" alt="ReLU" align=center /></p>
<p>p.s.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ReLU = lambda x: abs(x) * (x&gt;0)</span><br><span class="line">dReLU = lambda x: 1 * (x&gt;0)</span><br></pre></td></tr></table></figure>

<h4 id="ReLU-II"><a href="#ReLU-II" class="headerlink" title="ReLU (II)"></a>ReLU (II)</h4><p>ELU 函數是針對 ReLU 函數的一個改進型，相比於 ReLU 函數，在輸入為負數的情況下，是有一定的輸出的</p>
<ul>
<li>這樣可以消除 ReLU 死掉的問題</li>
<li>但還是有 <strong>梯度飽和</strong> 和 <strong>指數運算</strong> 的問題</li>
</ul>
<p>函式：<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899836437" width = "300" height = "200" alt="ELU fomula" align=center /><br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899845136" width = "400" height = "300" alt="ELU" align=center /></p>
<h4 id="ReLU-III"><a href="#ReLU-III" class="headerlink" title="ReLU (III)"></a>ReLU (III)</h4><ol>
<li>PReLU<br>參數化修正線性單元（Parameteric Rectified Linear Unit，PReLU）屬於 ReLU 修正類啟動函數的一員</li>
<li>Leaky ReLU<br>當 α&#x3D;0.1 時，我們叫 PReLU 為Leaky ReLU，算是 PReLU 的一種特殊情況</li>
<li>RReLU 以及 Leaky ReLU 有一些共同點，即爲負值輸入添加了一個線性項</li>
</ol>
<p>函式：<br>$$f(x)&#x3D;max(ax, x)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1593000067651" width = "400" height = "300" alt="" align=center /></p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout 是深度學習網路中的一層網路，就像池化層、卷積層一樣，可以看成是網路的啟動函數層<br>Maxout 神經元的啟動函數是取得所有這些「函數層」中的最大值<br>Maxout 的擬合能力是非​​常強的，<strong>優點</strong> 是計算簡單，不會過飽和，同時又沒有 ReLU 的缺點<br>Maxout 的 <strong>缺點</strong> 是過程參數相當於多了一倍</p>
<p>函式：<br>$$f(x)&#x3D;max(wT1x+b1, wT2x+b2)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592900004136" width = "400" height = "300" alt="" align=center /></p>
<h2 id="Sigmoid-vs-Tanh"><a href="#Sigmoid-vs-Tanh" class="headerlink" title="Sigmoid vs Tanh"></a>Sigmoid vs Tanh</h2><p>tanh 函數將輸入值壓縮到 -1~1 的範圍，因此它是 0 均值的，<br>這解決了Sigmoid 函數的非 zero-centered 問題，但是它也存在 <strong>梯度消失</strong> 和 <strong>冪運算</strong> 的問題。<br>其實 tanh(x)&#x3D;2sigmoid(2x)-1<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592900066747" width = "400" height = "300" alt="Sigmoid vs Tanh" align=center /><br>左邊是 Sigmoid 非線性函數，將實數壓縮到［0,1］之間。右邊是 Tanh 函數，將實數壓縮到［-1,1］。</p>
<h2 id="Sigmoid-vs-Softmax"><a href="#Sigmoid-vs-Softmax" class="headerlink" title="Sigmoid vs Softmax"></a>Sigmoid vs Softmax</h2><ul>
<li>Sigmoid 將一個 real value 映射到（0,1）的區間，用來做二分類</li>
<li>Softmax 把一個 k 維的 real value 向量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）其中 bi 是一個 0～1 的常數，輸出神經元之和為 1.0，所以可以拿來做多分類的機率預測</li>
<li>二分類問題時 sigmoid 和 softmax 是一樣的，求的都是 cross entropy loss</li>
</ul>
<h2 id="梯度消失-Vanishing-gradient-problem"><a href="#梯度消失-Vanishing-gradient-problem" class="headerlink" title="梯度消失 Vanishing gradient problem"></a>梯度消失 Vanishing gradient problem</h2><p>原因：前面的層比後面的層梯度變化更小，故變化更慢</p>
<p>結果：Output 變化慢 &#x3D;&gt; Gradient小 &#x3D;&gt; 學得慢  </p>
<p>Sigmoid，Tanh 都有這樣特性不適合用在 Layers 多的DNN 架構<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1593491298051" width = "400" height = "300" alt="梯度消失" align=center /></p>
<h2 id="如何選擇正確的啟動函數"><a href="#如何選擇正確的啟動函數" class="headerlink" title="如何選擇正確的啟動函數"></a>如何選擇正確的啟動函數</h2><h3 id="根據各個函數的優缺點來配置"><a href="#根據各個函數的優缺點來配置" class="headerlink" title="根據各個函數的優缺點來配置"></a>根據各個函數的優缺點來配置</h3><p>如果使用 ReLU，要小心設置 learning rate，注意不要讓網路出現很多「dead」 神經元，如果不好解決，可以試試 Leaky ReLU、PReLU 或者Maxout</p>
<p>p.s.<br>dead neurons的主因為神經元陷入永遠只能產生特定值，且有0個梯度(經常發生在ReLU)<br><a target="_blank" rel="noopener" href="https://medium.com/joelthchao/how-dead-neurons-hurt-training-5fc127d8db6a">更多</a></p>
<h3 id="根據問題的性質"><a href="#根據問題的性質" class="headerlink" title="根據問題的性質"></a>根據問題的性質</h3><ul>
<li>用於分類器時，Sigmoid 函數及其組合通常效果更好</li>
<li>由於梯度消失問題，有時要避免使用 sigmoid 和 tanh 函數。ReLU 函數是一個通用的啟動函數，目前在大多數情況下使用</li>
<li>如果神經網路中出現死神經元，那麼 PReLU 函數就是最好的選擇</li>
<li>ReLU 函數建議只能在隱藏層中使用</li>
</ul>
<h3 id="考慮-DNN-損失函數和啟動函數"><a href="#考慮-DNN-損失函數和啟動函數" class="headerlink" title="考慮 DNN 損失函數和啟動函數"></a>考慮 DNN 損失函數和啟動函數</h3><ul>
<li>如果使用 sigmoid 啟動函數，則交叉熵損失函數一般肯定比均方差損失函數好</li>
<li>如果是 DNN 用於分類，則一般在輸出層使用 softmax 啟動函數</li>
<li>ReLU 啟動函數對梯度消失問題有一定程度的解決，尤其是在 CNN模型中</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/27/%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/27/%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8/" class="post-title-link" itemprop="url">損失函數</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-27 10:12:06 / 修改時間：11:30:17" itemprop="dateCreated datePublished" datetime="2020-07-27T10:12:06+08:00">2020-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="損失函數"><a href="#損失函數" class="headerlink" title="損失函數"></a>損失函數</h1><p>機器學習中所有的算法都需要最大化或最小化一個函數，這個函數被稱為「目標函數」。其中，我們一般把最小化的一類函數，稱為「損失函數」。它能根據預測結果，衡量出模型預測能力的好壞</p>
<p>損失函數大致可分為：分類問題的損失函數和回歸問題的損失函數</p>
<p>$y$ 表示實際值，$\hat{y}$ 表示預測值</p>
<h2 id="均方誤差-MSE-mean-squared-error"><a href="#均方誤差-MSE-mean-squared-error" class="headerlink" title="均方誤差(MSE, mean_squared_error)"></a>均方誤差(MSE, mean_squared_error)</h2><p>就是最小平方法(Least Square) 的目標函數 – 預測值與實際值的差距之平均值。<br>還有其他變形的函數,<br>如 mean_absolute_error 、 mean_absolute_percentage_error 、 mean_squared_logarithmic_error等等</p>
<p>$$ \sum{(\hat{y} - y)^2 &#x2F; N}$$</p>
<h3 id="使用時機"><a href="#使用時機" class="headerlink" title="使用時機"></a>使用時機</h3><ul>
<li>n 個樣本的預測值（$\hat{y}$）與（$\hat{y}$）的差距</li>
<li>Numerical 相關</li>
</ul>
<h3 id="Keras-上的調用方式"><a href="#Keras-上的調用方式" class="headerlink" title="Keras 上的調用方式"></a>Keras 上的調用方式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= &#x27;mean_squared_error&#x27;, optimizer=&#x27;sgd&#x27;)</span><br><span class="line">其中，包含 y_true， y_pred 的傳遞，函數是表達如下：</span><br><span class="line">keras.losses.mean_squared_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>當預測值與實際值愈相近，損失函數就愈小，反之差距很大時，就會使損失函數愈小</p>
<p>為何要用 Cross Entropy 取代 MSE?<br>  因為，在梯度下降時，Cross Entropy 計算速度較快</p>
<h3 id="使用時機："><a href="#使用時機：" class="headerlink" title="使用時機："></a>使用時機：</h3><ul>
<li>整數目標：Sparse categorical_crossentropy</li>
<li>分類目標：categorical_crossentropy</li>
<li>二分類目標：binary_crossentropy</li>
</ul>
<h3 id="Keras-上的調用方式："><a href="#Keras-上的調用方式：" class="headerlink" title="Keras 上的調用方式："></a>Keras 上的調用方式：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= &#x27;categorical_crossentropy&#x27;, optimizer=&#x27;sgd&#x27;)</span><br><span class="line">其中, 包含y_true， y_pred的傳遞, 函數是表達如下：</span><br><span class="line">keras.losses.categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="Hinge-Error-hinge"><a href="#Hinge-Error-hinge" class="headerlink" title="Hinge Error (hinge)"></a>Hinge Error (hinge)</h2><p>是一種單邊誤差，不考慮負值。 同樣也有多種變形，squared_hinge、categorical_hinge</p>
<p>$$l(y) &#x3D; max(0, 1-t*y)$$</p>
<h3 id="使用時機：-1"><a href="#使用時機：-1" class="headerlink" title="使用時機："></a>使用時機：</h3><ul>
<li>適用於『支援向量機』(SVM)的最大間隔分類法(maximum-margin classification)</li>
</ul>
<h3 id="Keras-上的調用方式：-1"><a href="#Keras-上的調用方式：-1" class="headerlink" title="Keras 上的調用方式："></a>Keras 上的調用方式：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= ‘hinge‘, optimizer=&#x27;sgd’)</span><br><span class="line">其中，包含 y_true，y_pred 的傳遞, 函數是表達如下:</span><br><span class="line">keras.losses.hinge(y_true, y_pred) </span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/25/Keras-Sequential-API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/25/Keras-Sequential-API/" class="post-title-link" itemprop="url">Keras_Sequential_API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-25 11:54:52" itemprop="dateCreated datePublished" datetime="2020-07-25T11:54:52+08:00">2020-07-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-29 12:24:34" itemprop="dateModified" datetime="2020-07-29T12:24:34+08:00">2020-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h1><p>序列模型是多個網路層的線性堆疊。<br>Sequential 是一系列模型的簡單線性疊加，</p>
<ul>
<li><p>可以在構造函數中傳入一些列的網路層：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Dense, Activation</span><br><span class="line"></span><br><span class="line">model = Sequential(Dense(32, _input_shap=(784,), Activation(“relu”))</span><br></pre></td></tr></table></figure>
</li>
<li><p>也可以透過 .add</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(32, _input_dim=784))</span><br><span class="line">model.add(Activation(“relu”))</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Sequential-模型的基本元件"><a href="#Sequential-模型的基本元件" class="headerlink" title="Sequential 模型的基本元件"></a>Sequential 模型的基本元件</h2><p>Sequential 模型的基本元件一般需要：</p>
<ul>
<li>Model 宣告</li>
<li>model.add，添加層；</li>
<li>model.compile,模型訓練；</li>
<li>model.fit，模型訓練參數設置 + 訓練；</li>
<li>模型評估</li>
<li>模型預測</li>
</ul>
<p>例如<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294269/1593748586095" alt="sequential"></p>
<h2 id="keras-框架"><a href="#keras-框架" class="headerlink" title="keras 框架"></a>keras 框架</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294271/1593754429437" alt="image"></p>
<h2 id="指定模型的輸入維度"><a href="#指定模型的輸入維度" class="headerlink" title="指定模型的輸入維度"></a>指定模型的輸入維度</h2><p>Sequential 的第一層(只有第一層，後面的層會自動匹配)需要知道輸入的shape(<strong>input_shape</strong>)</p>
<ul>
<li>在第一層加入一個 input_shape 參數，input_shape 應該是一個 shape 的 tuple 資料類型。</li>
<li>input_shape 是一系列整數的 tuple，某些位置可為 None</li>
<li>input_shape 中不用指明 batch_size 的數目。</li>
</ul>
<p>2D 的網路層，如 Dense，允許在層的構造函數的 input_dim 中指定輸入的維度。<br>對於某些 3D 時間層，可以在構造函數中指定 input_dim 和 input_length 來實現。<br>對於某些 RNN，可以指定 batch_size。這樣後面的輸入必須是(batch_size, input_shape)的輸入</p>
<h2 id="常用參數說明"><a href="#常用參數說明" class="headerlink" title="常用參數說明"></a>常用參數說明</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294269/1593748569180" alt="often used"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/25/Keras-Module-API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/25/Keras-Module-API/" class="post-title-link" itemprop="url">Keras Module API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-25 11:33:06 / 修改時間：20:26:27" itemprop="dateCreated datePublished" datetime="2020-07-25T11:33:06+08:00">2020-07-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="函數式API"><a href="#函數式API" class="headerlink" title="函數式API"></a>函數式API</h1><p>定義復雜模型（如多輸出模型、有向無環圖，或具有共享層的模型）的方法</p>
<p>所有的模型都可調用，就像網絡層一樣</p>
<ul>
<li>利用函數式API，可以輕易地重用訓練好的模型：可以將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意，在調用模型時，您不僅重用模型的結構，還重用了它的權重</li>
</ul>
<p>範例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = input(shape(784,))</span><br><span class="line">y = model(x)</span><br></pre></td></tr></table></figure>

<h2 id="函數式API-與-序列模型"><a href="#函數式API-與-序列模型" class="headerlink" title="函數式API 與 序列模型"></a>函數式API 與 序列模型</h2><p>模型需要多於一個的輸出，應該選擇函數式模型<br>函數式模型是最廣泛的一類模型，序列模型（<a href="/2020/07/25/Keras-Sequential-API">Sequential</a>）只是它的一種特殊情況</p>
<p>利用函數式 API，可以輕易地重用訓練好的模型：可以將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意，在調用模型時，您不僅重用模型的結構，還重用了它的權重<br>具有多個輸入和輸出的模型。函數式 API 使處理大量交織的數據流變得容易</p>
<p>共享網路層 </p>
<ul>
<li>函數式API 的另一個用途是使用共享網絡層的模型</li>
<li>我們來看看共享層<br>來考慮推特推文數據集。<br>我們想要建立一個模型來分辨兩條推文是否來自同一個人（例如，通過推文的相似性來對用戶進行比較）<br>實現這個目標的一種方法是建立一個模型，將兩條推文編碼成兩個向量，連接向量，然後添加邏輯回歸層；<br>這將輸出兩條推文來自同一作者的概率。模型將接收一對對正負表示的推特數據<br>由於這個問題是對稱的，編碼第一條推文的機制應該被完全重用來編碼第二條推文（權重及其他全部）</li>
</ul>
<h2 id="keras-框架"><a href="#keras-框架" class="headerlink" title="keras 框架"></a>keras 框架</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294271/1593754429437" alt="image"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/02/%E9%9B%86%E6%88%90Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/02/%E9%9B%86%E6%88%90Ensemble/" class="post-title-link" itemprop="url">集成Ensemble</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-02 14:06:28" itemprop="dateCreated datePublished" datetime="2020-07-02T14:06:28+08:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-03 11:25:36" itemprop="dateModified" datetime="2020-07-03T11:25:36+08:00">2020-07-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="什麼是集成-ensemble"><a href="#什麼是集成-ensemble" class="headerlink" title="什麼是集成(ensemble)"></a>什麼是集成(ensemble)</h1><p>集成是使⽤不同⽅式，結合多個&#x2F;多種不同分類器，作為綜合預測的做法統稱。<br>通過對模型截長補短的方式，來取得更好的分數</p>
<p>其中⼜分為 <strong>資料⾯的集成</strong> : 如裝袋法(Bagging) &#x2F; 提升法(Boosting)<br>以及 <strong>模型與特徵的集成</strong> : 如混合泛化(Blending) &#x2F; 堆疊泛化(Stacking)</p>
<h2 id="資料⾯集成-裝袋法-Bagging"><a href="#資料⾯集成-裝袋法-Bagging" class="headerlink" title="資料⾯集成 : 裝袋法 ( Bagging )"></a>資料⾯集成 : 裝袋法 ( Bagging )</h2><p>裝袋法顧名思義，是將資料放入袋中抽取，每回合結束後全部放回袋中重抽<br>再搭配弱分類器取平均&#x2F;多數決結果，最有名的就是前⾯學過的 <strong>隨機森林(Random Forest)</strong></p>
<h2 id="資料⾯集成-提升法-Boosting"><a href="#資料⾯集成-提升法-Boosting" class="headerlink" title="資料⾯集成 : 提升法 ( Boosting )"></a>資料⾯集成 : 提升法 ( Boosting )</h2><p>提升法則是由之前模型的預測結果，去改變資料被抽到的權重或⽬標值<br>將錯判資料被抽中的機率放⼤，正確的縮⼩，就是 <strong>⾃適應提升 (AdaBoost,Adaptive Boosting)</strong><br>如果是依照估計誤差的殘差項調整新⽬標值，則就是 <strong>梯度提升機 (GradientBoosting Machine)</strong> 的作法，只是梯度提升機還加上⽤梯度來選擇決策樹分⽀</p>
<h2 id="資料集成-v-s-模型與特徵集成"><a href="#資料集成-v-s-模型與特徵集成" class="headerlink" title="資料集成 v.s. 模型與特徵集成"></a>資料集成 v.s. 模型與特徵集成</h2><p>值得一提的是雖然兩者都稱為集成，其實適⽤範圍差異很⼤，通常不會⼀起提及<br>但為了避免混淆，在這邊將兩者做個對比</p>
<h3 id="資料集成"><a href="#資料集成" class="headerlink" title="資料集成"></a>資料集成</h3><p>Bagging &#x2F; Boosting</p>
<ul>
<li>使⽤不同訓練資料 + 同⼀種模型，多次估計的結果合成最終預測</li>
</ul>
<h3 id="模型與特徵集成"><a href="#模型與特徵集成" class="headerlink" title="模型與特徵集成"></a>模型與特徵集成</h3><p>Voting &#x2F; Blending &#x2F; Stacking</p>
<ul>
<li>使⽤同⼀資料 + 不同模型，合成出不同預測結果</li>
</ul>
<h2 id="混合泛化-Blending"><a href="#混合泛化-Blending" class="headerlink" title="混合泛化 (Blending)"></a>混合泛化 (Blending)</h2><p>其實混合泛化非常單純，就是將不同模型的預測值 <strong>加權合成</strong> ，權重和為 1<br>如果取預測的平均 or ⼀⼈⼀票多數決(每個模型權重相同)，則⼜稱為 投票泛化(Voting)<br>混合泛化提升預測⼒的原因是基於模型差異度⼤，在預測細節上能互補，因此預測模型只要各⾃調參優化過且原理不同，通常都能使⽤混合泛化集成</p>
<h3 id="注意事項"><a href="#注意事項" class="headerlink" title="注意事項"></a>注意事項</h3><p>雖然blending可以有效的提升成績，然而 Blending 的前提是 : 個別單模效果都很好(有調參)並且模型差異⼤，<br>其中單模要好尤其重要，如果單模效果差異太⼤，Blending 的效果提升就相當有限</p>
<h3 id="延伸："><a href="#延伸：" class="headerlink" title="延伸："></a>延伸：</h3><p>林軒⽥老師公開課程中有更詳細的解說<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mjUKsp0MvMI&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=26">https://www.youtube.com/watch?v=mjUKsp0MvMI&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=26</a></p>
<h2 id="堆疊泛化-Stacking-原始paper"><a href="#堆疊泛化-Stacking-原始paper" class="headerlink" title="堆疊泛化(Stacking)  原始paper"></a>堆疊泛化(Stacking)  <a target="_blank" rel="noopener" href="http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf">原始paper</a></h2><h3 id="相對於-Blending-的改良"><a href="#相對於-Blending-的改良" class="headerlink" title="相對於 Blending 的改良"></a>相對於 Blending 的改良</h3><p>不只將預測結果混合，而是使用預測結果當新特徵<br>更進一步的運用了資料輔助集成，但也使得 Stacking 複雜許多</p>
<h3 id="Stacking-的設計挑戰-訓練測試的不可重複性"><a href="#Stacking-的設計挑戰-訓練測試的不可重複性" class="headerlink" title="Stacking 的設計挑戰 : 訓練測試的不可重複性"></a>Stacking 的設計挑戰 : 訓練測試的不可重複性</h3><p>Stacking 主要是把模型當作下一階的特徵編碼器來使用，但是待編碼資料與用來訓練編碼器的資料不可重複 (訓練測試的不可重複性)<br>若將訓練資料切成兩組 :<br>待編碼資料太少，下一層的資料筆數就會太少，<br>訓練編碼器的資料太少，則編碼器的強度就會不夠，</p>
<p>這樣的困境該如何解決呢?</p>
<h4 id="巧妙的-K-Fold-拆分"><a href="#巧妙的-K-Fold-拆分" class="headerlink" title="巧妙的 K-Fold 拆分"></a>巧妙的 K-Fold 拆分</h4><p>Stacking 最終採取 將資料拆成 K 份，每份含 1&#x2F;K 的資料，要編碼時，使用其他的 K-1 組資料訓練模型&#x2F;編碼器。<br>這樣資料就沒有變少，K 夠大時 編碼器的強韌性也夠，唯一的問題就是計算時間隨著 K 變大而變長，但 K 可以調整，且相對深度學習所需的時間來說，這樣的時間長度也還算可接受</p>
<h3 id="常見問題"><a href="#常見問題" class="headerlink" title="常見問題"></a>常見問題</h3><p>Q1：能不能新舊特徵一起用，再用模型預測呢?<br>A1：可以，這裡其實有個有趣的思考，也就是 : 這樣不就可以一直一直無限增加特徵下去?<br>這樣後面的特徵還有意義嗎? 不會 Overfitting 嗎?…<br>其實加太多次是會 Overfitting 的，因此必須謹慎切分 Fold 以及新增次數</p>
<p>Q2：新的特徵，能不能再搭配模型創造特徵，第三層第四層…⼀一直下去呢?<br>A2：可以，但是每多一層，模型會越複雜 : 因此泛化(又稱為魯棒性)會做得更好，精準度也會下降，<br>所以除非第一層的單模調得很好，否則兩三層就不需要繼續往下了了</p>
<p>Q3：既然同層新特徵會 Overfitting，層數加深會增加泛化，兩者同時用是不是就能把缺點互相抵銷呢?<br>A3：可以!! 而且這正是 Stacking 最有趣的地方，但真正實踐時，程式複雜，運算時間又要再往上一個量級，之前曾有⼤大神寫過 StackNet實現這個想法，用JVM 加速運算，但實際上使用時調參數困難，後繼使用的人就少了</p>
<h3 id="注意事項-1"><a href="#注意事項-1" class="headerlink" title="注意事項"></a>注意事項</h3><p>「分類問題」的 Stacking 要注意兩件事：記得加上 use_probas&#x3D;True(輸出特徵才會是機率值)，<br>以及輸出的總特徵數會是：模型數量*分類數量(回歸問題特徵數&#x3D;模型數量)</p>
<h3 id="相關"><a href="#相關" class="headerlink" title="相關"></a>相關</h3><p>StackingCVClassifier    - mlxtrend 官⽅方網站<br><a target="_blank" rel="noopener" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/02/%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E5%AF%AB%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/02/%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E5%AF%AB%E6%B3%95/" class="post-title-link" itemprop="url">可能用到寫法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-02 12:02:02" itemprop="dateCreated datePublished" datetime="2020-07-02T12:02:02+08:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-06 18:14:03" itemprop="dateModified" datetime="2020-07-06T18:14:03+08:00">2020-07-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="單純記錄下未來可能用到的寫法"><a href="#單純記錄下未來可能用到的寫法" class="headerlink" title="單純記錄下未來可能用到的寫法"></a>單純記錄下未來可能用到的寫法</h1><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h3 id="載入要用的libary"><a href="#載入要用的libary" class="headerlink" title="載入要用的libary"></a>載入要用的libary</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import copy, time</span><br><span class="line">from IPython.display import display</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.ensemble import GradientBoostingRegressor</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br></pre></td></tr></table></figure>

<h3 id="載入資料"><a href="#載入資料" class="headerlink" title="載入資料"></a>載入資料</h3><p>再載入資料後，對目標值做平滑化。接著拿掉id和目標值，並合併測試集及訓練集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data_path = &#x27;../data/&#x27;</span><br><span class="line">df_train = pd.read_csv(data_path + &#x27;house_train.csv.gz&#x27;)</span><br><span class="line">df_test = pd.read_csv(data_path + &#x27;house_test.csv.gz&#x27;)</span><br><span class="line"></span><br><span class="line">train_Y = np.log1p(df_train[&#x27;SalePrice&#x27;])</span><br><span class="line">ids = df_test[&#x27;Id&#x27;]</span><br><span class="line">df_train = df_train.drop([&#x27;Id&#x27;, &#x27;SalePrice&#x27;] , axis=1)</span><br><span class="line">df_test = df_test.drop([&#x27;Id&#x27;] , axis=1)</span><br><span class="line">df = pd.concat([df_train, df_test])</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<h3 id="特徵工程"><a href="#特徵工程" class="headerlink" title="特徵工程"></a>特徵工程</h3><h4 id="檢查-DataFrame-空缺值的狀態"><a href="#檢查-DataFrame-空缺值的狀態" class="headerlink" title="檢查 DataFrame 空缺值的狀態"></a>檢查 DataFrame 空缺值的狀態</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 檢查 DataFrame 空缺值的狀態</span><br><span class="line">def na_check(df_data):</span><br><span class="line">    data_na = (df_data.isnull().sum() / len(df_data)) * 100</span><br><span class="line">    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)</span><br><span class="line">    missing_data = pd.DataFrame(&#123;&#x27;Missing Ratio&#x27; :data_na&#125;)</span><br><span class="line">    display(missing_data.head(10))</span><br><span class="line">na_check(df)</span><br></pre></td></tr></table></figure>

<h4 id="將各column依不同類型分類"><a href="#將各column依不同類型分類" class="headerlink" title="將各column依不同類型分類"></a>將各column依不同類型分類</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#確定只有 int64, float64, object 三種類型後, 分別將欄位名稱存於三個 list 中</span><br><span class="line">int_features = []</span><br><span class="line">float_features = []</span><br><span class="line">object_features = []</span><br><span class="line">for dtype, feature in zip(df.dtypes, df.columns):</span><br><span class="line">    if dtype == &#x27;float64&#x27;:</span><br><span class="line">        float_features.append(feature)</span><br><span class="line">    elif dtype == &#x27;int64&#x27;:</span><br><span class="line">        int_features.append(feature)</span><br><span class="line">    else:</span><br><span class="line">        object_features.append(feature)</span><br><span class="line">print(f&#x27;&#123;len(int_features)&#125; Integer Features : &#123;int_features&#125;\n&#x27;)</span><br><span class="line">print(f&#x27;&#123;len(float_features)&#125; Float Features : &#123;float_features&#125;\n&#x27;)</span><br><span class="line">print(f&#x27;&#123;len(object_features)&#125; Object Features : &#123;object_features&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<h4 id="補空缺值"><a href="#補空缺值" class="headerlink" title="補空缺值"></a>補空缺值</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 部分欄位缺值補 &#x27;None&#x27;</span><br><span class="line">none_cols = [&#x27;PoolQC&#x27;, &#x27;MiscFeature&#x27;, &#x27;Alley&#x27;, &#x27;Fence&#x27;, &#x27;FireplaceQu&#x27;, &#x27;FireplaceQu&#x27;, &#x27;FireplaceQu&#x27;, &#x27;FireplaceQu&#x27;, </span><br><span class="line">            &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, </span><br><span class="line">             &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;MasVnrType&#x27;, &#x27;Functional&#x27;, &#x27;MSSubClass&#x27;]</span><br><span class="line">for col in none_cols:</span><br><span class="line">    df[col] = df[col].fillna(&quot;None&quot;)</span><br><span class="line">    </span><br><span class="line"># 部分欄位缺值填補 0</span><br><span class="line">zero_cols = [&#x27;GarageYrBlt&#x27;, &#x27;GarageArea&#x27;, &#x27;GarageCars&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;,&#x27;TotalBsmtSF&#x27;, </span><br><span class="line">             &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;MasVnrArea&#x27;]</span><br><span class="line">for col in zero_cols:</span><br><span class="line">    df[col] = df[col].fillna(0)</span><br><span class="line"></span><br><span class="line"># 部分欄位缺值補眾數</span><br><span class="line">mode_cols = [&#x27;MSZoning&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;SaleType&#x27;]</span><br><span class="line">for col in mode_cols:</span><br><span class="line">    df[col] = df[col].fillna(df[col].mode()[0])</span><br><span class="line">    </span><br><span class="line"># &#x27;LotFrontage&#x27; 有空缺時, 以同一區 (Neighborhood) 的 LotFrontage 中位數填補 (可以視為填補一種群聚編碼 )</span><br><span class="line">df[&quot;LotFrontage&quot;] = df.groupby(&quot;Neighborhood&quot;)[&quot;LotFrontage&quot;].transform(lambda x: x.fillna(x.median()))</span><br><span class="line"></span><br><span class="line"># Utilities 參考資訊很少, 所以直接捨棄</span><br><span class="line">df = df.drop([&#x27;Utilities&#x27;], axis=1)</span><br><span class="line"></span><br><span class="line"># 做完各種補缺值, 確認一下有沒有遺漏</span><br><span class="line">na_check(df)</span><br></pre></td></tr></table></figure>

<h4 id="做類別轉換-類別做LabelEncoding、數值轉類別"><a href="#做類別轉換-類別做LabelEncoding、數值轉類別" class="headerlink" title="做類別轉換(類別做LabelEncoding、數值轉類別)"></a>做類別轉換(類別做LabelEncoding、數值轉類別)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 四個數值欄位, 因為相異值有限, 轉成文字</span><br><span class="line">label_cols = [&#x27;MSSubClass&#x27;, &#x27;OverallCond&#x27;, &#x27;YrSold&#x27;, &#x27;MoSold&#x27;]</span><br><span class="line">for col in mode_cols:</span><br><span class="line">    df[col] = df[col].astype(str)</span><br><span class="line">    </span><br><span class="line"># 相異值不太具有代表性的, 做標籤編碼</span><br><span class="line">cols = (&#x27;FireplaceQu&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, </span><br><span class="line">        &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;,&#x27;HeatingQC&#x27;, &#x27;PoolQC&#x27;, &#x27;KitchenQual&#x27;, &#x27;BsmtFinType1&#x27;, </span><br><span class="line">        &#x27;BsmtFinType2&#x27;, &#x27;Functional&#x27;, &#x27;Fence&#x27;, &#x27;BsmtExposure&#x27;, &#x27;GarageFinish&#x27;, &#x27;LandSlope&#x27;,</span><br><span class="line">        &#x27;LotShape&#x27;, &#x27;PavedDrive&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;CentralAir&#x27;, &#x27;MSSubClass&#x27;, &#x27;OverallCond&#x27;, </span><br><span class="line">        &#x27;YrSold&#x27;, &#x27;MoSold&#x27;)</span><br><span class="line">for c in cols:</span><br><span class="line">    lbl = LabelEncoder() </span><br><span class="line">    lbl.fit(list(df[c].values)) </span><br><span class="line">    df[c] = lbl.transform(list(df[c].values))</span><br></pre></td></tr></table></figure>

<h4 id="確定關聯性"><a href="#確定關聯性" class="headerlink" title="確定關聯性"></a>確定關聯性</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">correlations = app_train.corr()[&#x27;TARGET&#x27;].sort_values()</span><br><span class="line"></span><br><span class="line"># 顯示相關係數最大 / 最小的各15個欄位名稱</span><br><span class="line">print(&#x27;Most Positive Correlations:\n&#x27;, correlations.tail(15))</span><br><span class="line">print(&#x27;\nMost Negative Correlations:\n&#x27;, correlations.head(15))</span><br></pre></td></tr></table></figure>

<h4 id="特徵組合"><a href="#特徵組合" class="headerlink" title="特徵組合"></a>特徵組合</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 由地下室面積 + 1樓面積 + 2樓面積, 計算總坪數特徵   </span><br><span class="line">df[&#x27;TotalSF&#x27;] = df[&#x27;TotalBsmtSF&#x27;] + df[&#x27;1stFlrSF&#x27;] + df[&#x27;2ndFlrSF&#x27;]    </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 看看目前特徵工程的結果</span><br><span class="line">print(&#x27;Shape df: &#123;&#125;&#x27;.format(df.shape))</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<h4 id="將剩下的類別型欄位做one-hot-encoding"><a href="#將剩下的類別型欄位做one-hot-encoding" class="headerlink" title="將剩下的類別型欄位做one hot encoding"></a>將剩下的類別型欄位做one hot encoding</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 把剩下少數重要的類別型欄位, 做獨熱編碼 (已變成數字的欄位, 會自動跳過)</span><br><span class="line">df = pd.get_dummies(df)</span><br><span class="line">print(df.shape)</span><br></pre></td></tr></table></figure>

<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><h4 id="將做完特徵工程的資料轉回train-x-test-x"><a href="#將做完特徵工程的資料轉回train-x-test-x" class="headerlink" title="將做完特徵工程的資料轉回train_x, test_x"></a>將做完特徵工程的資料轉回train_x, test_x</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 將前述轉換完畢資料 df , 重新切成 train_X, test_X</span><br><span class="line">train_num = train_Y.shape[0]</span><br><span class="line">train_X = df[:train_num]</span><br><span class="line">test_X = df[train_num:]</span><br></pre></td></tr></table></figure>

<h4 id="載入模型"><a href="#載入模型" class="headerlink" title="載入模型"></a>載入模型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 使用三種模型 : 線性迴歸 / 梯度提升機 / 隨機森林, 參數使用 Random Search 尋找</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor</span><br><span class="line">linear = LinearRegression(normalize=False, fit_intercept=True, copy_X=True)</span><br><span class="line">gdbt = GradientBoostingRegressor(tol=0.1, subsample=0.37, n_estimators=200, max_features=20, </span><br><span class="line">                                 max_depth=6, learning_rate=0.03)</span><br><span class="line">rf = RandomForestRegressor(n_estimators=300, min_samples_split=9, min_samples_leaf=10, </span><br><span class="line">                           max_features=&#x27;sqrt&#x27;, max_depth=8, bootstrap=False)</span><br></pre></td></tr></table></figure>

<h4 id="開始預測，並存為csv檔"><a href="#開始預測，並存為csv檔" class="headerlink" title="開始預測，並存為csv檔"></a>開始預測，並存為csv檔</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 線性迴歸預測檔</span><br><span class="line">linear.fit(train_X, train_Y)</span><br><span class="line">linear_pred = linear.predict(test_X)</span><br><span class="line">sub = pd.DataFrame(&#123;&#x27;Id&#x27;: ids, &#x27;SalePrice&#x27;: np.expm1(linear_pred)&#125;)</span><br><span class="line">sub.to_csv(&#x27;house_linear.csv&#x27;, index=False) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 梯度提升機預測檔 </span><br><span class="line">gdbt.fit(train_X, train_Y)</span><br><span class="line">gdbt_pred = gdbt.predict(test_X)</span><br><span class="line">sub = pd.DataFrame(&#123;&#x27;Id&#x27;: ids, &#x27;SalePrice&#x27;: np.expm1(gdbt_pred)&#125;)</span><br><span class="line">sub.to_csv(&#x27;house_gdbt.csv&#x27;, index=False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 隨機森林預測檔</span><br><span class="line">rf.fit(train_X, train_Y)</span><br><span class="line">rf_pred = rf.predict(test_X)</span><br><span class="line">sub = pd.DataFrame(&#123;&#x27;Id&#x27;: ids, &#x27;SalePrice&#x27;: np.expm1(rf_pred)&#125;)</span><br><span class="line">sub.to_csv(&#x27;house_rf.csv&#x27;, index=False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 混合泛化預測檔 (依 Kaggle 傳回分數調整比重, 越準確者比重越高, 依資料性質有所不同)</span><br><span class="line">blending_pred = linear_pred*0.30 + gdbt_pred*0.67 + rf_pred*0.03</span><br><span class="line">sub = pd.DataFrame(&#123;&#x27;Id&#x27;: ids, &#x27;SalePrice&#x27;: np.expm1(blending_pred)&#125;)</span><br><span class="line">sub.to_csv(&#x27;house_blending.csv&#x27;, index=False)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/30/%E8%B6%85%E5%8F%83%E6%95%B8%E8%AA%BF%E6%95%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/30/%E8%B6%85%E5%8F%83%E6%95%B8%E8%AA%BF%E6%95%B4/" class="post-title-link" itemprop="url">超參數調整</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-06-30 20:11:07" itemprop="dateCreated datePublished" datetime="2020-06-30T20:11:07+08:00">2020-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-02 22:21:53" itemprop="dateModified" datetime="2020-07-02T22:21:53+08:00">2020-07-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="超參數調整-hyper-parameter-optimization"><a href="#超參數調整-hyper-parameter-optimization" class="headerlink" title="超參數調整(hyper-parameter optimization)"></a>超參數調整(hyper-parameter optimization)</h1><p>之前接觸到的所有模型都有 超參數 需要設置，諸如</p>
<ul>
<li>LASSO，Ridge: α 的大小</li>
<li>決策樹：樹的深度、節點最小樣本數</li>
<li>隨機森林：樹的數量</li>
</ul>
<p>然而，雖然 超參數 會影響結果，但提升的效果有限，<br>將注意力放在資料清理與特徵工程才能最有效的提升準確率，而調整參數只是一個加分的工具而已</p>
<h2 id="超參數調整方法"><a href="#超參數調整方法" class="headerlink" title="超參數調整方法"></a>超參數調整方法</h2><ul>
<li>窮舉法 (Grid Search)：直接指定超參數的組合範圍，每一組參數都訓練完成，再根據驗證集 (validation) 的結果選擇最佳參數</li>
<li>隨機搜尋 (Random Search)：指定超參數的範圍，用均勻分布進行參數抽樣，用抽到的參數進行訓練，再根據驗證集的結果選擇最佳參數</li>
</ul>
<h3 id="隨機搜尋通常都能獲得更佳的結果-更多"><a href="#隨機搜尋通常都能獲得更佳的結果-更多" class="headerlink" title="隨機搜尋通常都能獲得更佳的結果 (更多)"></a>隨機搜尋通常都能獲得更佳的結果 (<a target="_blank" rel="noopener" href="https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881">更多</a>)</h3><p>在大多數情況下，目標函數的顛簸表面在所有維度上都不會那麼顛簸。有些參數對cost function的影響要比其他參數小得多，<strong>如果每個參數的重要性已知，則可以使用在grid seach每個參數選取值的數量中</strong>  但通常來說不是這樣的狀況。無論如何，__只要嘗試相同的次數，僅使用random search可以探索每個參數更多的值__， 進而帶來更好的結果。</p>
<h2 id="超參數調整步驟"><a href="#超參數調整步驟" class="headerlink" title="超參數調整步驟"></a>超參數調整步驟</h2><ol>
<li>先將資料切分為訓練&#x2F;測試集，測試集保留不使用</li>
<li>將剛切分好的訓練集，再使用Cross-validation 切分 K 份訓練&#x2F;驗證集</li>
<li>用 grid&#x2F;random search 的超參數進行訓練與評估</li>
<li>選出最佳的參數，用該參數與全部訓練集建模</li>
<li>最後使用測試集評估結果</li>
</ol>
<h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>Scanning hyperspace: how to tune machine learning models<br><a target="_blank" rel="noopener" href="https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/">https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/</a><br>Hyperparameter Tuning the Random Forest in Python<br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/23/%E8%A9%95%E4%BC%B0%E6%8C%87%E6%A8%99%E9%81%B8%E5%AE%9A-Evaluation-metrics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/23/%E8%A9%95%E4%BC%B0%E6%8C%87%E6%A8%99%E9%81%B8%E5%AE%9A-Evaluation-metrics/" class="post-title-link" itemprop="url">評估指標選定(Evaluation metrics)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-06-23 11:43:35 / 修改時間：14:43:28" itemprop="dateCreated datePublished" datetime="2020-06-23T11:43:35+08:00">2020-06-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="評估指標"><a href="#評估指標" class="headerlink" title="評估指標"></a>評估指標</h1><p>設定各項指標來評估模型預測的準確性，最常見的是<br>$$ 準確率(Accuracy) &#x3D; \frac{正確分類樣本數}{總樣本數} &#x3D; \frac{TP + TN}{P + N} $$<br>(TP:True positive, TN: True negative, P: Positive, N: Negative)</p>
<p><strong>sklearn 有相關lib</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import metrics</span><br></pre></td></tr></table></figure>

<h2 id="回歸"><a href="#回歸" class="headerlink" title="回歸"></a>回歸</h2><p>觀察「預測值」 (Prediction) 與「實際值」 (Ground truth) 的 <strong>差距</strong></p>
<ul>
<li>MAE, Mean Absolute Error, 範圍: [0, ∞]<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mae = metrics.mean_absolute_error(prediction, y) # 使用 MAE 評估</span><br></pre></td></tr></table></figure></li>
<li>MSE, Mean Square Error, 範圍: [0, ∞]<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mse = metrics.mean_squared_error(prediction, y) # 使用 MSE 評估</span><br></pre></td></tr></table></figure></li>
<li>R-square, 範圍: [0, 1] <strong>(常用)</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r2 = metrics.r2_score(prediction, y) # 使用 r-square 評估</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="分類"><a href="#分類" class="headerlink" title="分類"></a>分類</h2><p>觀察「預測值」 (prediction) 與「實際值」 (Ground truth) 的 <strong>正確程度</strong></p>
<ul>
<li>AUC, Area Under Curve, 範圍: [0, 1]</li>
<li>F1 - Score (Precision, Recall), 範圍: [0, 1]</li>
</ul>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC 指摽是分類問題常⽤的指標，通常分類問題都需要定⼀個閾值(threshold) 來決定分類的類別<br>(通常為機率 &gt; 0.5 判定為 1, 機率 &lt; 0.5 判定為 0)<br>而 AUC 是 <strong>衡量曲線下的⾯積</strong> ，因此可考量所有閾值下的準確性，在分類型比賽中常用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">auc = metrics.roc_auc_score(y_test, y_pred) # 使用 roc_auc_score 來評估。 </span><br><span class="line">## **這邊特別注意 y_pred 必須要放機率值進去!**</span><br></pre></td></tr></table></figure>


<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1-Score"></a>F1-Score</h3><p>分類問題中，我們有時會對某⼀類別的準確率特別有興趣。例如瑕疵&#x2F;正常樣本分類，我們希望任何瑕疵樣本都不能被漏掉。<br>(如判斷一個人是否有得病)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">threshold = 0.5 </span><br><span class="line">y_pred_binarized = np.where(y_pred&gt;threshold, 1, 0) # 使用 np.where 函數, 將 y_pred &gt; 0.5 的值變為 1，小於 0.5 的為 0</span><br></pre></td></tr></table></figure>

<p>Precision，Recall 則是針對某類別進⾏評估</p>
<ul>
<li><p>Precision: 模型判定瑕疵，樣本確實為瑕疵的比例		<br>$$ Precision &#x3D; \frac{TP}{TP+FP} $$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision = metrics.precision_score(y_test, y_pred_binarized) # 使用 Precision 評估</span><br></pre></td></tr></table></figure>
</li>
<li><p>Recall: 模型判定的瑕疵，佔樣本所有瑕疵的比例<br>$$ Recall &#x3D; \frac{TP}{TP+FN} $$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recall  = metrics.recall_score(y_test, y_pred_binarized) # 使用 recall 評估</span><br></pre></td></tr></table></figure>
<p>(以瑕疵檢測為例，若為 recall&#x3D;1 則代表所有瑕疵都被找到)</p>
</li>
</ul>
<p>F1-Score 則是 Precision, Recall 的調和平均數<br>$$ F1Score &#x3D; \frac{2 * Precision * Recall} {Precision + Recall} $$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f1 = metrics.f1_score(y_test, y_pred_binarized) # 使用 F1-Score 評估</span><br></pre></td></tr></table></figure>

<p>PS,<br>F1-Score 其實是 F-Score 中的 β 值為 1 的特例，代表 Precision 與 Recall 的權重相同<br>標準公式如下<br>$$ F_\beta &#x3D; (1 + \beta^2) * \frac{Precision * Recall} {(\beta^2 * Precision) + Recall}$$</p>
<h3 id="混淆矩陣-Confusion-Matrix"><a href="#混淆矩陣-Confusion-Matrix" class="headerlink" title="混淆矩陣 (Confusion Matrix)"></a>混淆矩陣 (Confusion Matrix)</h3><p>縱軸為模型預測，橫軸為正確答案<br>可以清楚看出每個 Class 間預測的準確率，完美的模型就會在對⾓線上呈現 100 % 的準確率<br><img src="https://www.mathworks.com/help/examples/nnet/win64/PlotConfusionMatrixUsingCategoricalLabelsExample_02.png" width="50%" height="50%"><center><a target="_blank" rel="noopener" href="https://www.mathworks.com/help/deeplearning/ref/plotconfusion.html">mathwork</a></center></p>
<h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>深入了解超常⽤的指標 AUC<br><a target="_blank" rel="noopener" href="https://www.dataschool.io/roc-curves-and-auc-explained/">https://www.dataschool.io/roc-curves-and-auc-explained/</a><br>學習更多評估指標，來衡量機器學習模型的準確度<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30721429">https://zhuanlan.zhihu.com/p/30721429</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/22/Regression-vs-Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/Regression-vs-Classification/" class="post-title-link" itemprop="url">Regression vs. Classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-06-22 13:39:45" itemprop="dateCreated datePublished" datetime="2020-06-22T13:39:45+08:00">2020-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-06-23 21:48:26" itemprop="dateModified" datetime="2020-06-23T21:48:26+08:00">2020-06-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="機器學習的監督式學習中主要分為"><a href="#機器學習的監督式學習中主要分為" class="headerlink" title="機器學習的監督式學習中主要分為"></a>機器學習的監督式學習中主要分為</h1><ul>
<li><p>回歸問題<br>回歸代表預測的⽬標值為實數 (-∞ ⾄ ∞)</p>
</li>
<li><p>分類問題。<br>分類代表預測的⽬標值為類別 (0 或 1)</p>
</li>
</ul>
<h2 id="目的差異"><a href="#目的差異" class="headerlink" title="目的差異"></a>目的差異</h2><p><strong>Supervised classification</strong> 的目的是要找出decision boundary, 也就是新得到的一筆 <strong>資料</strong> 是在decision boundary(邊界)的哪一邊來決定賦予哪一種class label.<br>而 <strong>Regression</strong> 的目的, 則是找出那條最能夠區分所有資料的那條 <strong>“線”</strong> ，再找所求的點對應的線上位置<br>(一個找點的分類，一個找分類線本身)</p>
<p><a target="_blank" rel="noopener" href="http://zylix666.blogspot.com/2016/06/supervised-classificationregression.html">更多</a></p>
<h1 id="⼆元分類-binary-class-vs-多元分類-Multi-class"><a href="#⼆元分類-binary-class-vs-多元分類-Multi-class" class="headerlink" title="⼆元分類 (binary-class) vs. 多元分類 (Multi-class)"></a>⼆元分類 (binary-class) vs. 多元分類 (Multi-class)</h1><ul>
<li><p><strong>二元分類</strong> ，顧名思義就是目標的類別僅有兩個(0 或 1)。<br>如垃圾郵件(垃圾郵件 vs. 非垃圾郵件)、瑕疵偵測 (瑕疵 vs. 正常)</p>
</li>
<li><p><strong>多元分類</strong> ，則是目標類別有兩種以上。<br>如手寫數字辨識有 10 個類別(0 ~ 9)</p>
</li>
</ul>
<h1 id="Multi-class-vs-Multi-label"><a href="#Multi-class-vs-Multi-label" class="headerlink" title="Multi-class vs. Multi-label"></a>Multi-class vs. Multi-label</h1><p>當每個樣本都只能歸在⼀個類別，我們稱之為 <strong>多分類 (Multi-class)</strong> 問題</p>
<p>而一個樣本如果可以同時有多個類別，則稱為 <strong>多標籤 (Multi-label)</strong> 問題</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Shoghi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shoghi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 強力驅動
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
