<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Shoghi的隨手筆記">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Shoghi的隨手筆記">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Shoghi">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-TW'
  };
</script>

  <title>Shoghi的隨手筆記</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Shoghi的隨手筆記</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首頁</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>歸檔</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/08/03/%E6%B7%B1%E9%80%A0%E6%BC%94%E8%AC%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/03/%E6%B7%B1%E9%80%A0%E6%BC%94%E8%AC%9B/" class="post-title-link" itemprop="url">深造演講</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-08-03 16:25:53" itemprop="dateCreated datePublished" datetime="2020-08-03T16:25:53+08:00">2020-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-08-08 09:14:53" itemprop="dateModified" datetime="2020-08-08T09:14:53+08:00">2020-08-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%B4%E5%93%88%E4%BC%8A/" itemprop="url" rel="index"><span itemprop="name">巴哈伊</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Day-1"><a href="#Day-1" class="headerlink" title="Day 1"></a>Day 1</h1><h2 id="上帝的目的"><a href="#上帝的目的" class="headerlink" title="上帝的目的"></a>上帝的目的</h2><p>上帝的目的在於創生萬物，在每個時期祂都會為人間帶來新的訊息。也就是顯聖者和他們所啟示的經書。<br>就像你在購買手機時會一併附給你的說明書一樣，你可以通過說明書快速了解到產品的意義，及他當中所潛藏的能力。<br>經書就是我們的說明書，為我們闡明了生命的意義，及我們的潛能。</p>
<h2 id="Bahaullah-is-he-who-says-he-is"><a href="#Bahaullah-is-he-who-says-he-is" class="headerlink" title="Bahaullah is he who says he is"></a>Bahaullah is he who says he is</h2><p>巴哈歐拉就是他自身所描述的那位。閱讀他的經典與他建立連結不是他人的責任，而是你自身的責任。 </p>
<h2 id="什麼是intelligent"><a href="#什麼是intelligent" class="headerlink" title="什麼是intelligent"></a>什麼是intelligent</h2><p>intelligent代表能夠讓蓄意的使事情發生，而非意外。<br>也就是他理解事物的本質，知曉如何將事務引導到他所渴望的結果中。</p>
<h2 id="何為personal-god"><a href="#何為personal-god" class="headerlink" title="何為personal god?"></a>何為personal god?</h2><blockquote>
<p>God is a god with conscious of his creation, who has a mind, a will, a purpose.<br>Not like most scientists think that god is unconscious and determined forces.<br>上帝是擁有意識、有思考、意志和目標的，而不是像大多科學家所想，無意識且堅決的力量。</p>
</blockquote>
<h2 id="若精神不存在於你的大腦或心臟，那精神究竟在哪裡呢？"><a href="#若精神不存在於你的大腦或心臟，那精神究竟在哪裡呢？" class="headerlink" title="若精神不存在於你的大腦或心臟，那精神究竟在哪裡呢？"></a>若精神不存在於你的大腦或心臟，那精神究竟在哪裡呢？</h2><p>精神存在於精神世界中，透過類似投影的方式在操作著肉體。</p>
<h2 id="靈魂"><a href="#靈魂" class="headerlink" title="靈魂"></a>靈魂</h2><p>在這個世界上除了人類為還有許多不同的靈魂種類，分別有礦物靈、植物靈、動物靈、人靈、信仰靈、跟聖靈</p>
<h3 id="各自的特性"><a href="#各自的特性" class="headerlink" title="各自的特性"></a>各自的特性</h3><p>下方靈魂同時擁有上方靈魂的特性</p>
<table>
<thead>
<tr>
<th><strong>靈魂種類</strong></th>
<th><strong>特性</strong></th>
</tr>
</thead>
<tbody><tr>
<td>礦物靈</td>
<td>凝聚力</td>
</tr>
<tr>
<td>植物靈</td>
<td>成長</td>
</tr>
<tr>
<td>動物靈</td>
<td>感官</td>
</tr>
<tr>
<td>人靈</td>
<td>思想</td>
</tr>
<tr>
<td>信仰靈</td>
<td>承認</td>
</tr>
<tr>
<td>聖靈</td>
<td>顯示</td>
</tr>
</tbody></table>
<h2 id="何謂思考呢？"><a href="#何謂思考呢？" class="headerlink" title="何謂思考呢？"></a>何謂思考呢？</h2><p>思考是個由未知到已知的過程。有趣的是人們唯有在擁有選擇的狀況下才有辦法思考。<br>因此面對常見的問題「若真的萬物皆為神所造，那為何神要創造那些不好的事情呢？」<br>答案就變得很明顯了，若人的一生中沒有了傷痛，那又怎麼能領悟健康的美好。<br>而毫無疑問的在人的一生最重要的便是死亡，若人不會死世間的一切便失去了意義。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/30/BackPropagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/30/BackPropagation/" class="post-title-link" itemprop="url">BackPropagation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-30 10:55:44" itemprop="dateCreated datePublished" datetime="2020-07-30T10:55:44+08:00">2020-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-08-03 16:43:30" itemprop="dateModified" datetime="2020-08-03T16:43:30+08:00">2020-08-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="反向傳播（BP：Backpropagation）"><a href="#反向傳播（BP：Backpropagation）" class="headerlink" title="反向傳播（BP：Backpropagation）"></a>反向傳播（BP：Backpropagation）</h1><p>反向傳播（BP：Backpropagation）是「誤差反向傳播」的簡稱，是一種與最優化方法（如梯度下降法）結合使用的該方法對網路中所有權重計算損失函數的梯度。這個梯度會反饋給最優化方法，用來更新權值以最小化損失函數</p>
<h2 id="執行方式"><a href="#執行方式" class="headerlink" title="執行方式"></a>執行方式</h2><h3 id="第-1-階段：解函數微分"><a href="#第-1-階段：解函數微分" class="headerlink" title="第 1 階段：解函數微分"></a>第 1 階段：解函數微分</h3><p>每次疊代中的傳播環節包含兩步： </p>
<ul>
<li>(前向傳播階段）將訓練輸入送入網路以獲得啟動響應</li>
<li>(反向傳播階段）將啟動響應同訓練輸入對應的目標輸出求差，從而獲得輸出層和隱藏層的響應誤差</li>
</ul>
<img src='https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294283/1592903273524' width=400 height=300 alt='FP/BP' align="center">

<h3 id="第-2-階段：權重更新"><a href="#第-2-階段：權重更新" class="headerlink" title="第 2 階段：權重更新"></a>第 2 階段：權重更新</h3><p>Follow Gradient Descent </p>
<ul>
<li>第 1 和第 2 階段可以反覆循環疊代，直到網路對輸入的響應達到滿意的預定的目標範圍為止</li>
</ul>
<h3 id="演算法"><a href="#演算法" class="headerlink" title="演算法"></a>演算法</h3><img src='https://miro.medium.com/max/700/1*nevYe3P-US1Ipj2PmRKQ5A.png' width=400 height=300 alt='BP algorithm' align="center">

<h2 id="優缺點"><a href="#優缺點" class="headerlink" title="優缺點"></a>優缺點</h2><h3 id="優點："><a href="#優點：" class="headerlink" title="優點："></a>優點：</h3><ul>
<li>具有任意複雜的模式分類能力和優良的多維函數映射能力，解決了簡單感知器不能解決的異或或者一些其他的問題</li>
<li>從結構上講，BP 神經網路具有輸入層、隱含層和輸出層</li>
<li>從本質上講，BP 算法就是以網路誤差平方目標函數、採用梯度下降法來計算目標函數的最小值</li>
</ul>
<h3 id="缺點："><a href="#缺點：" class="headerlink" title="缺點："></a>缺點：</h3><ul>
<li>學習速度慢，即使是一個簡單的過程，也需要幾百次甚至上千次的學習才能收斂</li>
<li>容易陷入局部極小值</li>
<li>網路層數、神經元個數的選擇沒有相應的理論指導</li>
<li>網路推廣能力有限</li>
</ul>
<h3 id="應用："><a href="#應用：" class="headerlink" title="應用："></a>應用：</h3><ul>
<li>函數逼近</li>
<li>模式識別 </li>
<li>分類</li>
<li>數據壓縮</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/29/Gradient-Descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/29/Gradient-Descent/" class="post-title-link" itemprop="url">Gradient-Descent</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-29 12:23:42 / 修改時間：16:43:24" itemprop="dateCreated datePublished" datetime="2020-07-29T12:23:42+08:00">2020-07-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient-Descent"></a>梯度下降 Gradient-Descent</h1><p>機器學習算法當中，優化算法的功能，是通過改善訓練方式，來最小化(或最大化)損失函數</p>
<h2 id="最常用的優化算法是Gradient-Descent"><a href="#最常用的優化算法是Gradient-Descent" class="headerlink" title="最常用的優化算法是Gradient Descent"></a>最常用的優化算法是Gradient Descent</h2><p>Gradient descent 是一個一階最佳化算法，通常也稱為最速下降法<br>目的：沿著目標函數梯度下降的方向搜索極小值（也可以沿著梯度上升的方向搜索極大值）<br>若要使用梯度下降法找到一個函數的局部極小值，必須向函數上當前點對應梯度（或者是近似梯度）的反方向的規定步長距離點進行疊代搜索。</p>
<h2 id="梯度下降法的缺點包括："><a href="#梯度下降法的缺點包括：" class="headerlink" title="梯度下降法的缺點包括："></a>梯度下降法的缺點包括：</h2><ul>
<li>靠近極小值時速度減慢</li>
<li>直線搜索可能會產生一些問題</li>
<li>可能會「之字型」地下降</li>
</ul>
<h2 id="學習率對梯度下降的影響"><a href="#學習率對梯度下降的影響" class="headerlink" title="學習率對梯度下降的影響"></a>學習率對梯度下降的影響</h2><p>學習率定義了每次疊代中應該更改的參數量。換句話說，它控制我們應該收斂到minimum的速度或速度<br>小學習率可以使迭代收斂，過大的學習率可能超過最小值<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294279/1592900888254" width = "400" height = "300" alt="learning rate" align=center /></p>
<h2 id="梯度下降法的過程"><a href="#梯度下降法的過程" class="headerlink" title="梯度下降法的過程"></a>梯度下降法的過程</h2><ol>
<li>首先需要設定一個初始參數值，通常情況下將初值設為零(w&#x3D;0)，</li>
<li>接下來需要計算成本函數 cost</li>
<li>然後計算函數的導數(某個點處的斜率值)，並設定學習效率參數(lr)的值</li>
<li>重複執行上述過程，直到參數值收斂，這樣我們就能獲得函數的最優解<img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294279/1592900924087" width = "400" height = "300" alt="gd-1" align=center /></li>
</ol>
<h2 id="要計算-Gradient-Descent，考慮"><a href="#要計算-Gradient-Descent，考慮" class="headerlink" title="要計算 Gradient Descent，考慮"></a>要計算 Gradient Descent，考慮</h2><ul>
<li>Loss &#x3D; 實際 ydata – 預測 ydata<br>Loss &#x3D;  w* 實際 xdata – w*預測 xdata (bias 為 init value，被消除)</li>
<li>Gradient &#x3D; ▽f (θ) (Gradient &#x3D; ∂L&#x2F;∂w)</li>
<li>調整後的權重 &#x3D; 原權重 – η(Learning rate) * Gradient</li>
</ul>
<h2 id="怎麼確定到極值點了呢？"><a href="#怎麼確定到極值點了呢？" class="headerlink" title="怎麼確定到極值點了呢？"></a>怎麼確定到極值點了呢？</h2><p>η又稱學習率，是一個挪動步長的基數，$\partial f(x)&#x2F;\partial x$是導函數，<br>當離極值很遠的時候導數大，移動的就快，當接近極值時，導數非常小，移動的就非常小</p>
<p><strong>然而，gradient descent並不保證可以達到global minimum，不同的起始點可以到達的minimum是不同的</strong></p>
<h2 id="梯度下降的算法調優"><a href="#梯度下降的算法調優" class="headerlink" title="梯度下降的算法調優"></a>梯度下降的算法調優</h2><p>Learning rate 選擇，實際上取值取決於數據樣本，如果損失函數在變小，說明取值有效，否則要增大 Learning rate</p>
<h2 id="自動更新-Learning-rate-衰減因子-decay"><a href="#自動更新-Learning-rate-衰減因子-decay" class="headerlink" title="自動更新 Learning rate  - 衰減因子 decay"></a>自動更新 Learning rate  - 衰減因子 decay</h2><p>算法參數的初始值選擇。初始值不同，獲得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；當然如果損失函數是凸函數則一定是最優解</p>
<h3 id="學習率衰減公式"><a href="#學習率衰減公式" class="headerlink" title="學習率衰減公式"></a>學習率衰減公式</h3><p>$$ lr_i &#x3D; lr_{start} * 1.0 &#x2F; (1.0 + decay * i)$$<br>其中 $lr_i$ 為第一迭代 i 時的學習率，$lr_start$ 為初始值，decay 為一個介於[0.0, 1.0]的小數。從公式上可看出：<br>decay 越小，學習率衰減地越慢，當 decay &#x3D; 0時，學習率保持不變<br>decay 越大，學習率衰減地越快，當 decay &#x3D; 1時，學習率衰減最快</p>
<h3 id="使用-momentum-是梯度下降法中一種常用的加速技術"><a href="#使用-momentum-是梯度下降法中一種常用的加速技術" class="headerlink" title="使用 momentum 是梯度下降法中一種常用的加速技術"></a>使用 momentum 是梯度下降法中一種常用的加速技術</h3><p>$x ← x − \alpha ∗ \partial x $(x沿負梯度方向下降)<br>$v &#x3D;  \beta ∗ v − a ∗ \partial x$<br>$x ← x + v$</p>
<p>其中 $\beta$ 即 momentum 係數，通俗的理解上面式子就是，如果上一次的momentum（即$\beta$ ）與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，所以這樣做能夠達到加速收斂的過程 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/27/activation-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/27/activation-function/" class="post-title-link" itemprop="url">activation function</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-27 17:23:09" itemprop="dateCreated datePublished" datetime="2020-07-27T17:23:09+08:00">2020-07-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-29 12:24:21" itemprop="dateModified" datetime="2020-07-29T12:24:21+08:00">2020-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="啟動函數-activation-function"><a href="#啟動函數-activation-function" class="headerlink" title="啟動函數 activation function"></a>啟動函數 activation function</h1><p>啟動函數定義了<br>每個節點（神經元）的輸出和輸入關係的函數為神經元提供規模化非線性化能力，讓神經網路具備強大的擬合能力</p>
<h2 id="啟動函數的作用"><a href="#啟動函數的作用" class="headerlink" title="啟動函數的作用"></a>啟動函數的作用</h2><ul>
<li><p>深度學習的基本原理是基於人工神經網路，信號從一個神經元進入，經過非線性的 activation function<br>如此循環往復，直到輸出層。正是由於這些非線性函數的反覆疊加，才使得神經網路有足夠的 capacity 來抓取複雜的 pattern</p>
</li>
<li><p>啟動函數的最大作用就是非線性化<br>如果不用啟動函數的話，無論神經網路有多少層，輸出都是輸入的線性組合</p>
</li>
<li><p>啟動函數的另一個重要特徵<br>它應該是可以區分forward-propagation與back-propagation的網路參數更新，然後相應地使用梯度下降或任何其他優化技術優化權重以減少誤差</p>
</li>
</ul>
<h2 id="常見啟動函數介紹"><a href="#常見啟動函數介紹" class="headerlink" title="常見啟動函數介紹"></a>常見啟動函數介紹</h2><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>特點是會把輸出限定在 0~1 之間，在 x&lt;0 ，輸出就是 0，在 x&gt;0，輸出就是 1，這樣使得數據在傳遞過程中不容易發散</p>
<h4 id="主要缺點"><a href="#主要缺點" class="headerlink" title="主要缺點"></a>主要缺點</h4><ul>
<li>Sigmoid 容易過飽和，丟失梯度。這樣在反向傳播時，很容易出現梯度消失的情況，導致訓練無法完整</li>
<li>Sigmoid 的輸出均值不是 0</li>
</ul>
<p>函式：<br>$$\sigma(x) &#x3D; \frac{1}{1+e^{-x}}$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899630177" width = "400" height = "300" alt="sigmoid" align=center /></p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>Softmax 把一個 k 維的 real value 向量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）其中 bi 是一個 0~1 的常數，輸出神經元之和為 1.0，所以也可以拿來做多分類的機率預測</p>
<h4 id="為什麼要取指數？"><a href="#為什麼要取指數？" class="headerlink" title="為什麼要取指數？"></a>為什麼要取指數？</h4><ul>
<li>要模擬 max 的行為，所以要讓大的更大</li>
<li>需要一個可導的函數</li>
</ul>
<p>函式：<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899676578" width = "300" height = "200" alt="softmax fomula" align=center /><br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899693493" width = "400" height = "300" alt="softmax" align=center /></p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>tanh 讀作 Hyperbolic Tangent<br>tanh 也稱為雙切正切函數，取值範圍為 [-1,1]</p>
<p>tanh 在特徵相差明顯時的效果會很好，在循環過程中會不斷擴大特徵效果</p>
<p>函式：<br>$$tanh(x)&#x3D;2\sigma(2x) - 1$$<br>$$tanh(x) &#x3D; \frac{e^x - e^{-x}}{e^x+e^{-x}}$$</p>
<img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899747636" width = "400" height = "300" alt="tanh" align=center />


<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>修正線性單元（Rectified linear unit，ReLU）</p>
<ul>
<li>在 x &gt; 0 時導數恆為 1</li>
<li>對於 x &lt; 0，其梯度恆為 0，這時候它也會出現飽和的現象，甚至使神經元直接無效，從而其權重無法得到更新（在這種情況下通常稱為 dying ReLU）</li>
<li>Leaky ReLU 和 PReLU 的提出正是為了解決這一問題</li>
</ul>
<p>函式：<br>$$f(x)&#x3D;max(0, x)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899800436" width = "400" height = "300" alt="ReLU" align=center /></p>
<p>p.s.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ReLU = lambda x: abs(x) * (x&gt;0)</span><br><span class="line">dReLU = lambda x: 1 * (x&gt;0)</span><br></pre></td></tr></table></figure>

<h4 id="ReLU-II"><a href="#ReLU-II" class="headerlink" title="ReLU (II)"></a>ReLU (II)</h4><p>ELU 函數是針對 ReLU 函數的一個改進型，相比於 ReLU 函數，在輸入為負數的情況下，是有一定的輸出的</p>
<ul>
<li>這樣可以消除 ReLU 死掉的問題</li>
<li>但還是有 <strong>梯度飽和</strong> 和 <strong>指數運算</strong> 的問題</li>
</ul>
<p>函式：<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899836437" width = "300" height = "200" alt="ELU fomula" align=center /><br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592899845136" width = "400" height = "300" alt="ELU" align=center /></p>
<h4 id="ReLU-III"><a href="#ReLU-III" class="headerlink" title="ReLU (III)"></a>ReLU (III)</h4><ol>
<li>PReLU<br>參數化修正線性單元（Parameteric Rectified Linear Unit，PReLU）屬於 ReLU 修正類啟動函數的一員</li>
<li>Leaky ReLU<br>當 α&#x3D;0.1 時，我們叫 PReLU 為Leaky ReLU，算是 PReLU 的一種特殊情況</li>
<li>RReLU 以及 Leaky ReLU 有一些共同點，即爲負值輸入添加了一個線性項</li>
</ol>
<p>函式：<br>$$f(x)&#x3D;max(ax, x)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1593000067651" width = "400" height = "300" alt="" align=center /></p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout 是深度學習網路中的一層網路，就像池化層、卷積層一樣，可以看成是網路的啟動函數層<br>Maxout 神經元的啟動函數是取得所有這些「函數層」中的最大值<br>Maxout 的擬合能力是非​​常強的，<strong>優點</strong> 是計算簡單，不會過飽和，同時又沒有 ReLU 的缺點<br>Maxout 的 <strong>缺點</strong> 是過程參數相當於多了一倍</p>
<p>函式：<br>$$f(x)&#x3D;max(wT1x+b1, wT2x+b2)$$<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592900004136" width = "400" height = "300" alt="" align=center /></p>
<h2 id="Sigmoid-vs-Tanh"><a href="#Sigmoid-vs-Tanh" class="headerlink" title="Sigmoid vs Tanh"></a>Sigmoid vs Tanh</h2><p>tanh 函數將輸入值壓縮到 -1~1 的範圍，因此它是 0 均值的，<br>這解決了Sigmoid 函數的非 zero-centered 問題，但是它也存在 <strong>梯度消失</strong> 和 <strong>冪運算</strong> 的問題。<br>其實 tanh(x)&#x3D;2sigmoid(2x)-1<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1592900066747" width = "400" height = "300" alt="Sigmoid vs Tanh" align=center /><br>左邊是 Sigmoid 非線性函數，將實數壓縮到［0,1］之間。右邊是 Tanh 函數，將實數壓縮到［-1,1］。</p>
<h2 id="Sigmoid-vs-Softmax"><a href="#Sigmoid-vs-Softmax" class="headerlink" title="Sigmoid vs Softmax"></a>Sigmoid vs Softmax</h2><ul>
<li>Sigmoid 將一個 real value 映射到（0,1）的區間，用來做二分類</li>
<li>Softmax 把一個 k 維的 real value 向量（a1,a2,a3,a4….）映射成一個（b1,b2,b3,b4….）其中 bi 是一個 0～1 的常數，輸出神經元之和為 1.0，所以可以拿來做多分類的機率預測</li>
<li>二分類問題時 sigmoid 和 softmax 是一樣的，求的都是 cross entropy loss</li>
</ul>
<h2 id="梯度消失-Vanishing-gradient-problem"><a href="#梯度消失-Vanishing-gradient-problem" class="headerlink" title="梯度消失 Vanishing gradient problem"></a>梯度消失 Vanishing gradient problem</h2><p>原因：前面的層比後面的層梯度變化更小，故變化更慢</p>
<p>結果：Output 變化慢 &#x3D;&gt; Gradient小 &#x3D;&gt; 學得慢  </p>
<p>Sigmoid，Tanh 都有這樣特性不適合用在 Layers 多的DNN 架構<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294277/1593491298051" width = "400" height = "300" alt="梯度消失" align=center /></p>
<h2 id="如何選擇正確的啟動函數"><a href="#如何選擇正確的啟動函數" class="headerlink" title="如何選擇正確的啟動函數"></a>如何選擇正確的啟動函數</h2><h3 id="根據各個函數的優缺點來配置"><a href="#根據各個函數的優缺點來配置" class="headerlink" title="根據各個函數的優缺點來配置"></a>根據各個函數的優缺點來配置</h3><p>如果使用 ReLU，要小心設置 learning rate，注意不要讓網路出現很多「dead」 神經元，如果不好解決，可以試試 Leaky ReLU、PReLU 或者Maxout</p>
<p>p.s.<br>dead neurons的主因為神經元陷入永遠只能產生特定值，且有0個梯度(經常發生在ReLU)<br><a target="_blank" rel="noopener" href="https://medium.com/joelthchao/how-dead-neurons-hurt-training-5fc127d8db6a">更多</a></p>
<h3 id="根據問題的性質"><a href="#根據問題的性質" class="headerlink" title="根據問題的性質"></a>根據問題的性質</h3><ul>
<li>用於分類器時，Sigmoid 函數及其組合通常效果更好</li>
<li>由於梯度消失問題，有時要避免使用 sigmoid 和 tanh 函數。ReLU 函數是一個通用的啟動函數，目前在大多數情況下使用</li>
<li>如果神經網路中出現死神經元，那麼 PReLU 函數就是最好的選擇</li>
<li>ReLU 函數建議只能在隱藏層中使用</li>
</ul>
<h3 id="考慮-DNN-損失函數和啟動函數"><a href="#考慮-DNN-損失函數和啟動函數" class="headerlink" title="考慮 DNN 損失函數和啟動函數"></a>考慮 DNN 損失函數和啟動函數</h3><ul>
<li>如果使用 sigmoid 啟動函數，則交叉熵損失函數一般肯定比均方差損失函數好</li>
<li>如果是 DNN 用於分類，則一般在輸出層使用 softmax 啟動函數</li>
<li>ReLU 啟動函數對梯度消失問題有一定程度的解決，尤其是在 CNN模型中</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/27/%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/27/%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8/" class="post-title-link" itemprop="url">損失函數</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-27 10:12:06 / 修改時間：11:30:17" itemprop="dateCreated datePublished" datetime="2020-07-27T10:12:06+08:00">2020-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="損失函數"><a href="#損失函數" class="headerlink" title="損失函數"></a>損失函數</h1><p>機器學習中所有的算法都需要最大化或最小化一個函數，這個函數被稱為「目標函數」。其中，我們一般把最小化的一類函數，稱為「損失函數」。它能根據預測結果，衡量出模型預測能力的好壞</p>
<p>損失函數大致可分為：分類問題的損失函數和回歸問題的損失函數</p>
<p>$y$ 表示實際值，$\hat{y}$ 表示預測值</p>
<h2 id="均方誤差-MSE-mean-squared-error"><a href="#均方誤差-MSE-mean-squared-error" class="headerlink" title="均方誤差(MSE, mean_squared_error)"></a>均方誤差(MSE, mean_squared_error)</h2><p>就是最小平方法(Least Square) 的目標函數 – 預測值與實際值的差距之平均值。<br>還有其他變形的函數,<br>如 mean_absolute_error 、 mean_absolute_percentage_error 、 mean_squared_logarithmic_error等等</p>
<p>$$ \sum{(\hat{y} - y)^2 &#x2F; N}$$</p>
<h3 id="使用時機"><a href="#使用時機" class="headerlink" title="使用時機"></a>使用時機</h3><ul>
<li>n 個樣本的預測值（$\hat{y}$）與（$\hat{y}$）的差距</li>
<li>Numerical 相關</li>
</ul>
<h3 id="Keras-上的調用方式"><a href="#Keras-上的調用方式" class="headerlink" title="Keras 上的調用方式"></a>Keras 上的調用方式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= &#x27;mean_squared_error&#x27;, optimizer=&#x27;sgd&#x27;)</span><br><span class="line">其中，包含 y_true， y_pred 的傳遞，函數是表達如下：</span><br><span class="line">keras.losses.mean_squared_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>當預測值與實際值愈相近，損失函數就愈小，反之差距很大時，就會使損失函數愈小</p>
<p>為何要用 Cross Entropy 取代 MSE?<br>  因為，在梯度下降時，Cross Entropy 計算速度較快</p>
<h3 id="使用時機："><a href="#使用時機：" class="headerlink" title="使用時機："></a>使用時機：</h3><ul>
<li>整數目標：Sparse categorical_crossentropy</li>
<li>分類目標：categorical_crossentropy</li>
<li>二分類目標：binary_crossentropy</li>
</ul>
<h3 id="Keras-上的調用方式："><a href="#Keras-上的調用方式：" class="headerlink" title="Keras 上的調用方式："></a>Keras 上的調用方式：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= &#x27;categorical_crossentropy&#x27;, optimizer=&#x27;sgd&#x27;)</span><br><span class="line">其中, 包含y_true， y_pred的傳遞, 函數是表達如下：</span><br><span class="line">keras.losses.categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="Hinge-Error-hinge"><a href="#Hinge-Error-hinge" class="headerlink" title="Hinge Error (hinge)"></a>Hinge Error (hinge)</h2><p>是一種單邊誤差，不考慮負值。 同樣也有多種變形，squared_hinge、categorical_hinge</p>
<p>$$l(y) &#x3D; max(0, 1-t*y)$$</p>
<h3 id="使用時機：-1"><a href="#使用時機：-1" class="headerlink" title="使用時機："></a>使用時機：</h3><ul>
<li>適用於『支援向量機』(SVM)的最大間隔分類法(maximum-margin classification)</li>
</ul>
<h3 id="Keras-上的調用方式：-1"><a href="#Keras-上的調用方式：-1" class="headerlink" title="Keras 上的調用方式："></a>Keras 上的調用方式：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras import losses</span><br><span class="line">model.compile(loss= ‘hinge‘, optimizer=&#x27;sgd’)</span><br><span class="line">其中，包含 y_true，y_pred 的傳遞, 函數是表達如下:</span><br><span class="line">keras.losses.hinge(y_true, y_pred) </span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/25/Keras-Sequential-API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/25/Keras-Sequential-API/" class="post-title-link" itemprop="url">Keras_Sequential_API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-25 11:54:52" itemprop="dateCreated datePublished" datetime="2020-07-25T11:54:52+08:00">2020-07-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-29 12:24:34" itemprop="dateModified" datetime="2020-07-29T12:24:34+08:00">2020-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h1><p>序列模型是多個網路層的線性堆疊。<br>Sequential 是一系列模型的簡單線性疊加，</p>
<ul>
<li><p>可以在構造函數中傳入一些列的網路層：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Dense, Activation</span><br><span class="line"></span><br><span class="line">model = Sequential(Dense(32, _input_shap=(784,), Activation(“relu”))</span><br></pre></td></tr></table></figure>
</li>
<li><p>也可以透過 .add</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(32, _input_dim=784))</span><br><span class="line">model.add(Activation(“relu”))</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Sequential-模型的基本元件"><a href="#Sequential-模型的基本元件" class="headerlink" title="Sequential 模型的基本元件"></a>Sequential 模型的基本元件</h2><p>Sequential 模型的基本元件一般需要：</p>
<ul>
<li>Model 宣告</li>
<li>model.add，添加層；</li>
<li>model.compile,模型訓練；</li>
<li>model.fit，模型訓練參數設置 + 訓練；</li>
<li>模型評估</li>
<li>模型預測</li>
</ul>
<p>例如<br><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294269/1593748586095" alt="sequential"></p>
<h2 id="keras-框架"><a href="#keras-框架" class="headerlink" title="keras 框架"></a>keras 框架</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294271/1593754429437" alt="image"></p>
<h2 id="指定模型的輸入維度"><a href="#指定模型的輸入維度" class="headerlink" title="指定模型的輸入維度"></a>指定模型的輸入維度</h2><p>Sequential 的第一層(只有第一層，後面的層會自動匹配)需要知道輸入的shape(<strong>input_shape</strong>)</p>
<ul>
<li>在第一層加入一個 input_shape 參數，input_shape 應該是一個 shape 的 tuple 資料類型。</li>
<li>input_shape 是一系列整數的 tuple，某些位置可為 None</li>
<li>input_shape 中不用指明 batch_size 的數目。</li>
</ul>
<p>2D 的網路層，如 Dense，允許在層的構造函數的 input_dim 中指定輸入的維度。<br>對於某些 3D 時間層，可以在構造函數中指定 input_dim 和 input_length 來實現。<br>對於某些 RNN，可以指定 batch_size。這樣後面的輸入必須是(batch_size, input_shape)的輸入</p>
<h2 id="常用參數說明"><a href="#常用參數說明" class="headerlink" title="常用參數說明"></a>常用參數說明</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294269/1593748569180" alt="often used"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/25/Keras-Module-API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/25/Keras-Module-API/" class="post-title-link" itemprop="url">Keras Module API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>
              

              <time title="創建時間：2020-07-25 11:33:06 / 修改時間：20:26:27" itemprop="dateCreated datePublished" datetime="2020-07-25T11:33:06+08:00">2020-07-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="函數式API"><a href="#函數式API" class="headerlink" title="函數式API"></a>函數式API</h1><p>定義復雜模型（如多輸出模型、有向無環圖，或具有共享層的模型）的方法</p>
<p>所有的模型都可調用，就像網絡層一樣</p>
<ul>
<li>利用函數式API，可以輕易地重用訓練好的模型：可以將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意，在調用模型時，您不僅重用模型的結構，還重用了它的權重</li>
</ul>
<p>範例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = input(shape(784,))</span><br><span class="line">y = model(x)</span><br></pre></td></tr></table></figure>

<h2 id="函數式API-與-序列模型"><a href="#函數式API-與-序列模型" class="headerlink" title="函數式API 與 序列模型"></a>函數式API 與 序列模型</h2><p>模型需要多於一個的輸出，應該選擇函數式模型<br>函數式模型是最廣泛的一類模型，序列模型（<a href="/2020/07/25/Keras-Sequential-API">Sequential</a>）只是它的一種特殊情況</p>
<p>利用函數式 API，可以輕易地重用訓練好的模型：可以將任何模型看作是一個層，然後通過傳遞一個張量來調用它。注意，在調用模型時，您不僅重用模型的結構，還重用了它的權重<br>具有多個輸入和輸出的模型。函數式 API 使處理大量交織的數據流變得容易</p>
<p>共享網路層 </p>
<ul>
<li>函數式API 的另一個用途是使用共享網絡層的模型</li>
<li>我們來看看共享層<br>來考慮推特推文數據集。<br>我們想要建立一個模型來分辨兩條推文是否來自同一個人（例如，通過推文的相似性來對用戶進行比較）<br>實現這個目標的一種方法是建立一個模型，將兩條推文編碼成兩個向量，連接向量，然後添加邏輯回歸層；<br>這將輸出兩條推文來自同一作者的概率。模型將接收一對對正負表示的推特數據<br>由於這個問題是對稱的，編碼第一條推文的機制應該被完全重用來編碼第二條推文（權重及其他全部）</li>
</ul>
<h2 id="keras-框架"><a href="#keras-框架" class="headerlink" title="keras 框架"></a>keras 框架</h2><p><img src="https://ai100-fileentity.cupoy.com/ml100/dailytask/1586225294271/1593754429437" alt="image"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/02/%E9%9B%86%E6%88%90Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/02/%E9%9B%86%E6%88%90Ensemble/" class="post-title-link" itemprop="url">集成Ensemble</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-02 14:06:28" itemprop="dateCreated datePublished" datetime="2020-07-02T14:06:28+08:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-03 11:25:36" itemprop="dateModified" datetime="2020-07-03T11:25:36+08:00">2020-07-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="什麼是集成-ensemble"><a href="#什麼是集成-ensemble" class="headerlink" title="什麼是集成(ensemble)"></a>什麼是集成(ensemble)</h1><p>集成是使⽤不同⽅式，結合多個&#x2F;多種不同分類器，作為綜合預測的做法統稱。<br>通過對模型截長補短的方式，來取得更好的分數</p>
<p>其中⼜分為 <strong>資料⾯的集成</strong> : 如裝袋法(Bagging) &#x2F; 提升法(Boosting)<br>以及 <strong>模型與特徵的集成</strong> : 如混合泛化(Blending) &#x2F; 堆疊泛化(Stacking)</p>
<h2 id="資料⾯集成-裝袋法-Bagging"><a href="#資料⾯集成-裝袋法-Bagging" class="headerlink" title="資料⾯集成 : 裝袋法 ( Bagging )"></a>資料⾯集成 : 裝袋法 ( Bagging )</h2><p>裝袋法顧名思義，是將資料放入袋中抽取，每回合結束後全部放回袋中重抽<br>再搭配弱分類器取平均&#x2F;多數決結果，最有名的就是前⾯學過的 <strong>隨機森林(Random Forest)</strong></p>
<h2 id="資料⾯集成-提升法-Boosting"><a href="#資料⾯集成-提升法-Boosting" class="headerlink" title="資料⾯集成 : 提升法 ( Boosting )"></a>資料⾯集成 : 提升法 ( Boosting )</h2><p>提升法則是由之前模型的預測結果，去改變資料被抽到的權重或⽬標值<br>將錯判資料被抽中的機率放⼤，正確的縮⼩，就是 <strong>⾃適應提升 (AdaBoost,Adaptive Boosting)</strong><br>如果是依照估計誤差的殘差項調整新⽬標值，則就是 <strong>梯度提升機 (GradientBoosting Machine)</strong> 的作法，只是梯度提升機還加上⽤梯度來選擇決策樹分⽀</p>
<h2 id="資料集成-v-s-模型與特徵集成"><a href="#資料集成-v-s-模型與特徵集成" class="headerlink" title="資料集成 v.s. 模型與特徵集成"></a>資料集成 v.s. 模型與特徵集成</h2><p>值得一提的是雖然兩者都稱為集成，其實適⽤範圍差異很⼤，通常不會⼀起提及<br>但為了避免混淆，在這邊將兩者做個對比</p>
<h3 id="資料集成"><a href="#資料集成" class="headerlink" title="資料集成"></a>資料集成</h3><p>Bagging &#x2F; Boosting</p>
<ul>
<li>使⽤不同訓練資料 + 同⼀種模型，多次估計的結果合成最終預測</li>
</ul>
<h3 id="模型與特徵集成"><a href="#模型與特徵集成" class="headerlink" title="模型與特徵集成"></a>模型與特徵集成</h3><p>Voting &#x2F; Blending &#x2F; Stacking</p>
<ul>
<li>使⽤同⼀資料 + 不同模型，合成出不同預測結果</li>
</ul>
<h2 id="混合泛化-Blending"><a href="#混合泛化-Blending" class="headerlink" title="混合泛化 (Blending)"></a>混合泛化 (Blending)</h2><p>其實混合泛化非常單純，就是將不同模型的預測值 <strong>加權合成</strong> ，權重和為 1<br>如果取預測的平均 or ⼀⼈⼀票多數決(每個模型權重相同)，則⼜稱為 投票泛化(Voting)<br>混合泛化提升預測⼒的原因是基於模型差異度⼤，在預測細節上能互補，因此預測模型只要各⾃調參優化過且原理不同，通常都能使⽤混合泛化集成</p>
<h3 id="注意事項"><a href="#注意事項" class="headerlink" title="注意事項"></a>注意事項</h3><p>雖然blending可以有效的提升成績，然而 Blending 的前提是 : 個別單模效果都很好(有調參)並且模型差異⼤，<br>其中單模要好尤其重要，如果單模效果差異太⼤，Blending 的效果提升就相當有限</p>
<h3 id="延伸："><a href="#延伸：" class="headerlink" title="延伸："></a>延伸：</h3><p>林軒⽥老師公開課程中有更詳細的解說<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mjUKsp0MvMI&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=26">https://www.youtube.com/watch?v=mjUKsp0MvMI&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=26</a></p>
<h2 id="堆疊泛化-Stacking-原始paper"><a href="#堆疊泛化-Stacking-原始paper" class="headerlink" title="堆疊泛化(Stacking)  原始paper"></a>堆疊泛化(Stacking)  <a target="_blank" rel="noopener" href="http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf">原始paper</a></h2><h3 id="相對於-Blending-的改良"><a href="#相對於-Blending-的改良" class="headerlink" title="相對於 Blending 的改良"></a>相對於 Blending 的改良</h3><p>不只將預測結果混合，而是使用預測結果當新特徵<br>更進一步的運用了資料輔助集成，但也使得 Stacking 複雜許多</p>
<h3 id="Stacking-的設計挑戰-訓練測試的不可重複性"><a href="#Stacking-的設計挑戰-訓練測試的不可重複性" class="headerlink" title="Stacking 的設計挑戰 : 訓練測試的不可重複性"></a>Stacking 的設計挑戰 : 訓練測試的不可重複性</h3><p>Stacking 主要是把模型當作下一階的特徵編碼器來使用，但是待編碼資料與用來訓練編碼器的資料不可重複 (訓練測試的不可重複性)<br>若將訓練資料切成兩組 :<br>待編碼資料太少，下一層的資料筆數就會太少，<br>訓練編碼器的資料太少，則編碼器的強度就會不夠，</p>
<p>這樣的困境該如何解決呢?</p>
<h4 id="巧妙的-K-Fold-拆分"><a href="#巧妙的-K-Fold-拆分" class="headerlink" title="巧妙的 K-Fold 拆分"></a>巧妙的 K-Fold 拆分</h4><p>Stacking 最終採取 將資料拆成 K 份，每份含 1&#x2F;K 的資料，要編碼時，使用其他的 K-1 組資料訓練模型&#x2F;編碼器。<br>這樣資料就沒有變少，K 夠大時 編碼器的強韌性也夠，唯一的問題就是計算時間隨著 K 變大而變長，但 K 可以調整，且相對深度學習所需的時間來說，這樣的時間長度也還算可接受</p>
<h3 id="常見問題"><a href="#常見問題" class="headerlink" title="常見問題"></a>常見問題</h3><p>Q1：能不能新舊特徵一起用，再用模型預測呢?<br>A1：可以，這裡其實有個有趣的思考，也就是 : 這樣不就可以一直一直無限增加特徵下去?<br>這樣後面的特徵還有意義嗎? 不會 Overfitting 嗎?…<br>其實加太多次是會 Overfitting 的，因此必須謹慎切分 Fold 以及新增次數</p>
<p>Q2：新的特徵，能不能再搭配模型創造特徵，第三層第四層…⼀一直下去呢?<br>A2：可以，但是每多一層，模型會越複雜 : 因此泛化(又稱為魯棒性)會做得更好，精準度也會下降，<br>所以除非第一層的單模調得很好，否則兩三層就不需要繼續往下了了</p>
<p>Q3：既然同層新特徵會 Overfitting，層數加深會增加泛化，兩者同時用是不是就能把缺點互相抵銷呢?<br>A3：可以!! 而且這正是 Stacking 最有趣的地方，但真正實踐時，程式複雜，運算時間又要再往上一個量級，之前曾有⼤大神寫過 StackNet實現這個想法，用JVM 加速運算，但實際上使用時調參數困難，後繼使用的人就少了</p>
<h3 id="注意事項-1"><a href="#注意事項-1" class="headerlink" title="注意事項"></a>注意事項</h3><p>「分類問題」的 Stacking 要注意兩件事：記得加上 use_probas&#x3D;True(輸出特徵才會是機率值)，<br>以及輸出的總特徵數會是：模型數量*分類數量(回歸問題特徵數&#x3D;模型數量)</p>
<h3 id="相關"><a href="#相關" class="headerlink" title="相關"></a>相關</h3><p>StackingCVClassifier    - mlxtrend 官⽅方網站<br><a target="_blank" rel="noopener" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/07/02/%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E5%AF%AB%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/02/%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E5%AF%AB%E6%B3%95/" class="post-title-link" itemprop="url">可能用到寫法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-07-02 12:02:02" itemprop="dateCreated datePublished" datetime="2020-07-02T12:02:02+08:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2022-05-22 20:02:40" itemprop="dateModified" datetime="2022-05-22T20:02:40+08:00">2022-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="單純記錄下未來可能用到的寫法"><a href="#單純記錄下未來可能用到的寫法" class="headerlink" title="單純記錄下未來可能用到的寫法"></a>單純記錄下未來可能用到的寫法</h1><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h3 id="載入要用的libary"><a href="#載入要用的libary" class="headerlink" title="載入要用的libary"></a>載入要用的libary</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import copy, time</span><br><span class="line">from IPython.display import display</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.ensemble import GradientBoostingRegressor</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/07/02/%E5%8F%AF%E8%83%BD%E7%94%A8%E5%88%B0%E5%AF%AB%E6%B3%95/#more" rel="contents">
                閱讀全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/30/%E8%B6%85%E5%8F%83%E6%95%B8%E8%AA%BF%E6%95%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shoghi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shoghi的隨手筆記">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/30/%E8%B6%85%E5%8F%83%E6%95%B8%E8%AA%BF%E6%95%B4/" class="post-title-link" itemprop="url">超參數調整</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-06-30 20:11:07" itemprop="dateCreated datePublished" datetime="2020-06-30T20:11:07+08:00">2020-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2020-07-02 22:21:53" itemprop="dateModified" datetime="2020-07-02T22:21:53+08:00">2020-07-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">程式筆記</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="超參數調整-hyper-parameter-optimization"><a href="#超參數調整-hyper-parameter-optimization" class="headerlink" title="超參數調整(hyper-parameter optimization)"></a>超參數調整(hyper-parameter optimization)</h1><p>之前接觸到的所有模型都有 超參數 需要設置，諸如</p>
<ul>
<li>LASSO，Ridge: α 的大小</li>
<li>決策樹：樹的深度、節點最小樣本數</li>
<li>隨機森林：樹的數量</li>
</ul>
<p>然而，雖然 超參數 會影響結果，但提升的效果有限，<br>將注意力放在資料清理與特徵工程才能最有效的提升準確率，而調整參數只是一個加分的工具而已</p>
<h2 id="超參數調整方法"><a href="#超參數調整方法" class="headerlink" title="超參數調整方法"></a>超參數調整方法</h2><ul>
<li>窮舉法 (Grid Search)：直接指定超參數的組合範圍，每一組參數都訓練完成，再根據驗證集 (validation) 的結果選擇最佳參數</li>
<li>隨機搜尋 (Random Search)：指定超參數的範圍，用均勻分布進行參數抽樣，用抽到的參數進行訓練，再根據驗證集的結果選擇最佳參數</li>
</ul>
<h3 id="隨機搜尋通常都能獲得更佳的結果-更多"><a href="#隨機搜尋通常都能獲得更佳的結果-更多" class="headerlink" title="隨機搜尋通常都能獲得更佳的結果 (更多)"></a>隨機搜尋通常都能獲得更佳的結果 (<a target="_blank" rel="noopener" href="https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881">更多</a>)</h3><p>在大多數情況下，目標函數的顛簸表面在所有維度上都不會那麼顛簸。有些參數對cost function的影響要比其他參數小得多，<strong>如果每個參數的重要性已知，則可以使用在grid seach每個參數選取值的數量中</strong>  但通常來說不是這樣的狀況。無論如何，__只要嘗試相同的次數，僅使用random search可以探索每個參數更多的值__， 進而帶來更好的結果。</p>
<h2 id="超參數調整步驟"><a href="#超參數調整步驟" class="headerlink" title="超參數調整步驟"></a>超參數調整步驟</h2><ol>
<li>先將資料切分為訓練&#x2F;測試集，測試集保留不使用</li>
<li>將剛切分好的訓練集，再使用Cross-validation 切分 K 份訓練&#x2F;驗證集</li>
<li>用 grid&#x2F;random search 的超參數進行訓練與評估</li>
<li>選出最佳的參數，用該參數與全部訓練集建模</li>
<li>最後使用測試集評估結果</li>
</ol>
<h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>Scanning hyperspace: how to tune machine learning models<br><a target="_blank" rel="noopener" href="https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/">https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/</a><br>Hyperparameter Tuning the Random Forest in Python<br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Shoghi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/shoghilin" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;shoghilin" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shoghiLin22@gmail.com" title="E-Mail → mailto:shoghiLin22@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/%E6%9F%8F%E5%AE%87-%E6%9E%97-03995a1a0" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;柏宇-林-03995a1a0" rel="noopener" target="_blank"><i class="linkedin-square fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shoghi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 強力驅動
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
